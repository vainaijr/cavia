{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cavia_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vainaijr/cavia/blob/master/cavia_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gle0AGFjDrdl",
        "colab_type": "text"
      },
      "source": [
        "# google drive mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA9nOhCWDuAe",
        "colab_type": "code",
        "outputId": "84f51736-bbd4-442c-806b-3816da2439f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBxfEomqEqxI",
        "colab_type": "code",
        "outputId": "75efad98-4acc-4879-c222-74f2a6e9bde2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "%xmode verbose\n",
        "%pdb on\n",
        "from IPython.core.debugger import set_trace\n",
        "import time\n",
        "from datetime import datetime\n",
        "!pip install torchsummaryX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Verbose\n",
            "Automatic pdb calling has been turned ON\n",
            "Collecting torchsummaryX\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/58/e6a19d2cd1784c16b43f3f9f4946fa5f084a1da3e8a758545cc95edb8fc0/torchsummaryX-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (0.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.16.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->torchsummaryX) (1.12.0)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjuK-js2CBnx",
        "colab_type": "text"
      },
      "source": [
        "# arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcDXl9LDB_3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description='Fast Context Adaptation via Meta-Learning (CAVIA),'\n",
        "                                                 'Regression experiments')\n",
        "\n",
        "    parser.add_argument('--task', type=str, default='sine', help='problem setting: sine or celeba')\n",
        "\n",
        "    parser.add_argument('--n_iter', type=int, default=50000, help='number of meta-iterations')\n",
        "\n",
        "    parser.add_argument('--tasks_per_metaupdate', type=int, default=25)\n",
        "\n",
        "    parser.add_argument('--k_meta_train', type=int, default=10, help='data points in task training set (during meta training, inner loop)')\n",
        "    parser.add_argument('--k_meta_test', type=int, default=10, help='data points in task test set (during meta training, outer loop)')\n",
        "    parser.add_argument('--k_shot_eval', type=int, default=10, help='data points in task training set (during evaluation)')\n",
        "\n",
        "    parser.add_argument('--lr_inner', type=float, default=1.0, help='inner-loop learning rate (task-specific)')\n",
        "    parser.add_argument('--lr_meta', type=float, default=0.001, help='outer-loop learning rate')\n",
        "\n",
        "    parser.add_argument('--num_inner_updates', type=int, default=1, help='number of inner-loop updates (during training)')\n",
        "\n",
        "    parser.add_argument('--num_context_params', type=int, default=5, help='number of context parameters (added at first layer)')\n",
        "    parser.add_argument('--num_hidden_layers', nargs='+', default=[40, 40])\n",
        "\n",
        "    parser.add_argument('--first_order', action='store_true', default=False, help='run first-order version')\n",
        "\n",
        "    parser.add_argument('--maml', action='store_true', default=False, help='run MAML')\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "\n",
        "    # commands specific to the CelebA image completion task\n",
        "    parser.add_argument('--use_ordered_pixels', action='store_true', default=False)\n",
        "\n",
        "    args = parser.parse_args('')\n",
        "\n",
        "    # use the GPU if available\n",
        "    args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    return args"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIbeqK3gCEwB",
        "colab_type": "text"
      },
      "source": [
        "# cavia_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRNsMZfuCGIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class CaviaModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-forward neural network with context parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_in,\n",
        "                 n_out,\n",
        "                 num_context_params,\n",
        "                 n_hidden,\n",
        "                 device\n",
        "                 ):\n",
        "        super(CaviaModel, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # fully connected layers\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        self.fc_layers.append(nn.Linear(n_in + num_context_params, n_hidden[0]))\n",
        "        for k in range(len(n_hidden) - 1):\n",
        "            self.fc_layers.append(nn.Linear(n_hidden[k], n_hidden[k + 1]))\n",
        "        self.fc_layers.append(nn.Linear(n_hidden[-1], n_out))\n",
        "\n",
        "        # context parameters (note that these are *not* registered parameters of the model!)\n",
        "        self.num_context_params = num_context_params\n",
        "        self.context_params = None\n",
        "        self.reset_context_params()\n",
        "\n",
        "    def reset_context_params(self):\n",
        "        self.context_params = torch.zeros(self.num_context_params).to(self.device)\n",
        "        self.context_params.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # set_trace()\n",
        "        \n",
        "        # x -> 10 x 1\n",
        "        \n",
        "        # concatenate input with context parameters\n",
        "        x = torch.cat((x, self.context_params.expand(x.shape[0], -1)), dim=1)\n",
        "        \n",
        "        # x -> 10 x 6\n",
        "        \n",
        "        for k in range(len(self.fc_layers) - 1):\n",
        "            x = F.relu(self.fc_layers[k](x))\n",
        "        \n",
        "        # x -> 10 x 40\n",
        "        \n",
        "        y = self.fc_layers[-1](x)\n",
        "        \n",
        "        # y -> 10 x 1\n",
        "        \n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6BkjobpCRRv",
        "colab_type": "text"
      },
      "source": [
        "# cavia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7o6D1tyCTeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Regression experiment using CAVIA\n",
        "\"\"\"\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def run(args, log_interval=5000, rerun=False):\n",
        "    assert not args.maml\n",
        "\n",
        "    # see if we already ran this experiment\n",
        "    code_root = os.path.dirname('/gdrive/My Drive/cavia/regression/' + datetime.now().strftime('%Y-%m-%d_%H_%M'))\n",
        "    if not os.path.isdir('{}/{}_result_files/{}'.format(code_root, args.task, datetime.now().strftime('%Y-%m-%d_%H_%M'))):\n",
        "        # os.mkdir('{}/'.format(code_root))\n",
        "        # os.mkdir('{}/{}_result_files/'.format(code_root, args.task))\n",
        "        os.mkdir('{}/{}_result_files/{}/'.format(code_root, args.task, datetime.now().strftime('%Y-%m-%d_%H_%M')))\n",
        "    path = '{}/{}_result_files/{}/'.format(code_root, args.task, datetime.now().strftime('%Y-%m-%d_%H_%M')) + get_path_from_args(args)\n",
        "\n",
        "    if os.path.exists(path + '.pkl') and not rerun:\n",
        "        return load_obj(path)\n",
        "\n",
        "    start_time = time.time()\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # --- initialise everything ---\n",
        "\n",
        "    # get the task family\n",
        "    if args.task == 'sine':\n",
        "        task_family_train = RegressionTasksSinusoidal()\n",
        "        task_family_valid = RegressionTasksSinusoidal()\n",
        "        task_family_test = RegressionTasksSinusoidal()\n",
        "    elif args.task == 'celeba':\n",
        "        task_family_train = tasks_celebA.CelebADataset('train', device=args.device)\n",
        "        task_family_valid = tasks_celebA.CelebADataset('valid', device=args.device)\n",
        "        task_family_test = tasks_celebA.CelebADataset('test', device=args.device)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # initialise network\n",
        "    model = CaviaModel(n_in=task_family_train.num_inputs,\n",
        "                       n_out=task_family_train.num_outputs,\n",
        "                       num_context_params=args.num_context_params,\n",
        "                       n_hidden=args.num_hidden_layers,\n",
        "                       device=args.device\n",
        "                       ).to(args.device)\n",
        "    \n",
        "    # intitialise meta-optimiser\n",
        "    # (only on shared params - context parameters are *not* registered parameters of the model)\n",
        "    !pip install adabound\n",
        "    import adabound\n",
        "    # meta_optimiser = torch.optim.Adam(model.parameters(), 0.001)\n",
        "    meta_optimiser = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
        "    \n",
        "    # meta_optimiser = optim.Adam(model.parameters(), args.lr_meta)\n",
        "    set_trace()\n",
        "    # initialise loggers\n",
        "    logger = Logger()\n",
        "    logger.best_valid_model = copy.deepcopy(model)\n",
        "\n",
        "    # --- main training loop ---\n",
        "\n",
        "    for i_iter in range(args.n_iter):\n",
        "\n",
        "        # initialise meta-gradient\n",
        "        meta_gradient = [0 for _ in range(len(model.state_dict()))]\n",
        "        \n",
        "        # sample tasks\n",
        "        target_functions = task_family_train.sample_tasks(args.tasks_per_metaupdate)\n",
        "\n",
        "        # --- inner loop ---\n",
        "\n",
        "        for t in range(args.tasks_per_metaupdate):\n",
        "\n",
        "            # reset private network weights\n",
        "            model.reset_context_params()\n",
        "\n",
        "            # get data for current task\n",
        "            train_inputs = task_family_train.sample_inputs(args.k_meta_train,\n",
        "                                                           args.use_ordered_pixels).to(args.device)\n",
        "\n",
        "            for _ in range(args.num_inner_updates):\n",
        "                \n",
        "                # forward through model\n",
        "                train_outputs = model(train_inputs)\n",
        "                # train_outpus -> 10 x 1\n",
        "                \n",
        "                # get targets\n",
        "                train_targets = target_functions[t](train_inputs)\n",
        "                # train_targets -> 10 x 1\n",
        "\n",
        "                # ------------ update on current task ------------\n",
        "\n",
        "                # compute loss for current task\n",
        "                task_loss = F.mse_loss(train_outputs, train_targets)\n",
        "\n",
        "                # compute gradient wrt context params\n",
        "                task_gradients = \\\n",
        "                    torch.autograd.grad(task_loss, model.context_params, create_graph=not args.first_order)[0]\n",
        "                # task_gradients -> 5 x 1\n",
        "                \n",
        "                # update context params (this will set up the computation graph correctly)\n",
        "                model.context_params = model.context_params - args.lr_inner * task_gradients\n",
        "                # model.context_params -> 5 x 1\n",
        "\n",
        "            # ------------ compute meta-gradient on test loss of current task ------------\n",
        "\n",
        "            # get test data\n",
        "            test_inputs = task_family_train.sample_inputs(args.k_meta_test, args.use_ordered_pixels).to(args.device)\n",
        "\n",
        "            # get outputs after update\n",
        "            test_outputs = model(test_inputs)\n",
        "\n",
        "            # get the correct targets\n",
        "            test_targets = target_functions[t](test_inputs)\n",
        "            \n",
        "\n",
        "            # compute loss after updating context (will backprop through inner loop)\n",
        "            loss_meta = F.mse_loss(test_outputs, test_targets)\n",
        "\n",
        "            # compute gradient + save for current task\n",
        "            task_grad = torch.autograd.grad(loss_meta, model.parameters())\n",
        "\n",
        "            for i in range(len(task_grad)):\n",
        "                # clip the gradient\n",
        "                meta_gradient[i] += task_grad[i].detach().clamp_(-10, 10)\n",
        "                \n",
        "\n",
        "        # ------------ meta update ------------\n",
        "\n",
        "        # assign meta-gradient\n",
        "        for i, param in enumerate(model.parameters()):\n",
        "            # param -> 40 x 6 -> 40 -> 40 x 40 -> 40 -> 1 x 40 -> 1\n",
        "            param.grad = meta_gradient[i] / args.tasks_per_metaupdate\n",
        "\n",
        "        # do update step on shared model\n",
        "        meta_optimiser.step()\n",
        "\n",
        "        # reset context params\n",
        "        model.reset_context_params()\n",
        "\n",
        "        # ------------ logging ------------\n",
        "\n",
        "        if i_iter % log_interval == 0:\n",
        "\n",
        "            # evaluate on training set\n",
        "            loss_mean, loss_conf = eval_cavia(args, copy.deepcopy(model), task_family=task_family_train,\n",
        "                                              num_updates=args.num_inner_updates)\n",
        "            logger.train_loss.append(loss_mean)\n",
        "            logger.train_conf.append(loss_conf)\n",
        "\n",
        "            # evaluate on test set\n",
        "            loss_mean, loss_conf = eval_cavia(args, copy.deepcopy(model), task_family=task_family_valid,\n",
        "                                              num_updates=args.num_inner_updates)\n",
        "            logger.valid_loss.append(loss_mean)\n",
        "            logger.valid_conf.append(loss_conf)\n",
        "\n",
        "            # evaluate on validation set\n",
        "            loss_mean, loss_conf = eval_cavia(args, copy.deepcopy(model), task_family=task_family_test,\n",
        "                                              num_updates=args.num_inner_updates)\n",
        "            logger.test_loss.append(loss_mean)\n",
        "            logger.test_conf.append(loss_conf)\n",
        "\n",
        "            # save logging results\n",
        "            save_obj(logger, path)\n",
        "\n",
        "            # save best model\n",
        "            if logger.valid_loss[-1] == np.min(logger.valid_loss):\n",
        "                print('saving best model at iter', i_iter)\n",
        "                logger.best_valid_model = copy.deepcopy(model)\n",
        "\n",
        "            # visualise results\n",
        "            if args.task == 'celeba':\n",
        "                tasks_celebA.visualise(task_family_train, task_family_test, copy.deepcopy(logger.best_valid_model),\n",
        "                                       args, i_iter)\n",
        "\n",
        "            # print current results\n",
        "            logger.print_info(i_iter, start_time)\n",
        "            start_time = time.time()\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "def eval_cavia(args, model, task_family, num_updates, n_tasks=100, return_gradnorm=False):\n",
        "\n",
        "    # get the task family\n",
        "    input_range = task_family.get_input_range().to(args.device)\n",
        "\n",
        "    # logging\n",
        "    losses = []\n",
        "    gradnorms = []\n",
        "\n",
        "    # --- inner loop ---\n",
        "\n",
        "    for t in range(n_tasks):\n",
        "\n",
        "        # sample a task\n",
        "        target_function = task_family.sample_task()\n",
        "\n",
        "        # reset context parameters\n",
        "        model.reset_context_params()\n",
        "\n",
        "        # get data for current task\n",
        "        curr_inputs = task_family.sample_inputs(args.k_shot_eval, args.use_ordered_pixels).to(args.device)\n",
        "        curr_targets = target_function(curr_inputs)\n",
        "\n",
        "        # ------------ update on current task ------------\n",
        "\n",
        "        for _ in range(1, num_updates + 1):\n",
        "\n",
        "            # forward pass\n",
        "            curr_outputs = model(curr_inputs)\n",
        "\n",
        "            # compute loss for current task\n",
        "            task_loss = F.mse_loss(curr_outputs, curr_targets)\n",
        "\n",
        "            # compute gradient wrt context params\n",
        "            task_gradients = \\\n",
        "                torch.autograd.grad(task_loss, model.context_params, create_graph=not args.first_order)[0]\n",
        "\n",
        "            # update context params\n",
        "            if args.first_order:\n",
        "                model.context_params = model.context_params - args.lr_inner * task_gradients.detach()\n",
        "            else:\n",
        "                model.context_params = model.context_params - args.lr_inner * task_gradients\n",
        "\n",
        "            # keep track of gradient norms\n",
        "            gradnorms.append(task_gradients[0].norm().item())\n",
        "\n",
        "        # ------------ logging ------------\n",
        "\n",
        "        # compute true loss on entire input range\n",
        "        model.eval()\n",
        "        losses.append(F.mse_loss(model(input_range), target_function(input_range)).detach().item())\n",
        "        model.train()\n",
        "\n",
        "    losses_mean = np.mean(losses)\n",
        "    losses_conf = st.t.interval(0.95, len(losses) - 1, loc=losses_mean, scale=st.sem(losses))\n",
        "    if not return_gradnorm:\n",
        "        return losses_mean, np.mean(np.abs(losses_conf - losses_mean))\n",
        "    else:\n",
        "        return losses_mean, np.mean(np.abs(losses_conf - losses_mean)), np.mean(gradnorms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5JJZ3FoCcG4",
        "colab_type": "text"
      },
      "source": [
        "# logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzhX3kyDCgVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Logger:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train_loss = []\n",
        "        self.train_conf = []\n",
        "\n",
        "        self.valid_loss = []\n",
        "        self.valid_conf = []\n",
        "\n",
        "        self.test_loss = []\n",
        "        self.test_conf = []\n",
        "\n",
        "        self.best_valid_model = None\n",
        "\n",
        "    def print_info(self, iter_idx, start_time):\n",
        "        print(\n",
        "            'Iter {:<4} - time: {:<5} - [train] loss: {:<6} (+/-{:<6}) - [valid] loss: {:<6} (+/-{:<6}) - [test] loss: {:<6} (+/-{:<6})'.format(\n",
        "                iter_idx,\n",
        "                int(time.time() - start_time),\n",
        "                np.round(self.train_loss[-1], 4),\n",
        "                np.round(self.train_conf[-1], 4),\n",
        "                np.round(self.valid_loss[-1], 4),\n",
        "                np.round(self.valid_conf[-1], 4),\n",
        "                np.round(self.test_loss[-1], 4),\n",
        "                np.round(self.test_conf[-1], 4),\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEBOr4iADB8U",
        "colab_type": "text"
      },
      "source": [
        "# regression_task_sinusoidal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZBNgqBRCn5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class RegressionTasksSinusoidal:\n",
        "    \"\"\"\n",
        "    Same regression task as in Finn et al. 2017 (MAML)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.num_inputs = 1\n",
        "        self.num_outputs = 1\n",
        "\n",
        "        self.amplitude_range = [0.1, 5.0]\n",
        "        self.phase_range = [0, np.pi]\n",
        "\n",
        "        self.input_range = [-5, 5]\n",
        "\n",
        "    def get_input_range(self, size=100):\n",
        "        return torch.linspace(self.input_range[0], self.input_range[1], steps=size).unsqueeze(1)\n",
        "\n",
        "    def sample_inputs(self, batch_size, *args, **kwargs):\n",
        "        inputs = torch.rand((batch_size, self.num_inputs))\n",
        "        inputs = inputs * (self.input_range[1] - self.input_range[0]) + self.input_range[0]\n",
        "        return inputs\n",
        "\n",
        "    def sample_task(self):\n",
        "        amplitude = np.random.uniform(self.amplitude_range[0], self.amplitude_range[1])\n",
        "        phase = np.random.uniform(self.phase_range[0], self.phase_range[1])\n",
        "        return self.get_target_function(amplitude, phase)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_target_function(amplitude, phase):\n",
        "        def target_function(x):\n",
        "            if isinstance(x, torch.Tensor):\n",
        "                return torch.sin(x - phase) * amplitude\n",
        "            else:\n",
        "                return np.sin(x - phase) * amplitude\n",
        "\n",
        "        return target_function\n",
        "\n",
        "    def sample_tasks(self, num_tasks, return_specs=False):\n",
        "\n",
        "        amplitude = np.random.uniform(self.amplitude_range[0], self.amplitude_range[1], num_tasks)\n",
        "        phase = np.random.uniform(self.phase_range[0], self.phase_range[1], num_tasks)\n",
        "\n",
        "        target_functions = []\n",
        "        for i in range(num_tasks):\n",
        "            target_functions.append(self.get_target_function(amplitude[i], phase[i]))\n",
        "\n",
        "        if return_specs:\n",
        "            return target_functions, amplitude, phase\n",
        "        else:\n",
        "            return target_functions\n",
        "\n",
        "    def sample_datapoints(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample random input/output pairs (e.g. for training an orcale)\n",
        "        :param batch_size:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        amplitudes = torch.Tensor(np.random.uniform(self.amplitude_range[0], self.amplitude_range[1], batch_size))\n",
        "        phases = torch.Tensor(np.random.uniform(self.phase_range[0], self.phase_range[1], batch_size))\n",
        "\n",
        "        inputs = torch.rand((batch_size, self.num_inputs))\n",
        "        inputs = inputs * (self.input_range[1] - self.input_range[0]) + self.input_range[0]\n",
        "        inputs = inputs.view(-1)\n",
        "\n",
        "        outputs = torch.sin(inputs - phases) * amplitudes\n",
        "        outputs = outputs.unsqueeze(1)\n",
        "\n",
        "        return torch.stack((inputs, amplitudes, phases)).t(), outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8zdG_gEDH5e",
        "colab_type": "text"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtuCDyXgCvl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def set_seed(seed, cudnn=True):\n",
        "    \"\"\"\n",
        "    Seed everything we can!\n",
        "    Note that gym environments might need additional seeding (env.seed(seed)),\n",
        "    and num_workers needs to be set to 1.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # note: the below slows down the code but makes it reproducible\n",
        "    if (seed is not None) and cudnn:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def get_path_from_args(args):\n",
        "    \"\"\" Returns a unique hash for an argparse object. \"\"\"\n",
        "    args_str = str(args)\n",
        "    path = hashlib.md5(args_str.encode()).hexdigest()\n",
        "    return path\n",
        "\n",
        "\n",
        "def get_base_path():\n",
        "    p = os.path.dirname(os.path.realpath(__file__))\n",
        "    if os.path.exists(p):\n",
        "        return p\n",
        "    raise RuntimeError('I dont know where I am; please specify a path for saving results.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s66v419sDddz",
        "colab_type": "text"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80MTOuvZDheJ",
        "colab_type": "code",
        "outputId": "c338df4d-1954-4ea6-9781-10063978fa70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    args = parse_args()\n",
        "\n",
        "    if args.maml:\n",
        "        logger = maml.run(args, log_interval=100, rerun=True)\n",
        "    else:\n",
        "        logger = run(args, log_interval=100, rerun=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabound\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/44/0c2c414effb3d9750d780b230dbb67ea48ddc5d9a6d7a9b7e6fcc6bdcff9/adabound-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.16.4)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n",
            "> \u001b[0;32m<ipython-input-12-65da013eafeb>\u001b[0m(64)\u001b[0;36mrun\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     62 \u001b[0;31m    \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     63 \u001b[0;31m    \u001b[0;31m# initialise loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 64 \u001b[0;31m    \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     65 \u001b[0;31m    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_valid_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     66 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "saving best model at iter 0\n",
            "Iter 0    - time: 14    - [train] loss: 4.6189 (+/-0.7951) - [valid] loss: 4.28   (+/-0.7393) - [test] loss: 4.06   (+/-0.775 )\n",
            "saving best model at iter 100\n",
            "Iter 100  - time: 8     - [train] loss: 3.5466 (+/-0.6137) - [valid] loss: 3.2269 (+/-0.5479) - [test] loss: 2.4946 (+/-0.4865)\n",
            "saving best model at iter 200\n",
            "Iter 200  - time: 8     - [train] loss: 2.1961 (+/-0.3914) - [valid] loss: 2.1626 (+/-0.4192) - [test] loss: 2.3    (+/-0.5065)\n",
            "saving best model at iter 300\n",
            "Iter 300  - time: 8     - [train] loss: 1.9519 (+/-0.3756) - [valid] loss: 1.7245 (+/-0.3568) - [test] loss: 1.4034 (+/-0.2602)\n",
            "saving best model at iter 400\n",
            "Iter 400  - time: 7     - [train] loss: 1.2106 (+/-0.2266) - [valid] loss: 1.0684 (+/-0.2114) - [test] loss: 0.8617 (+/-0.1976)\n",
            "saving best model at iter 500\n",
            "Iter 500  - time: 8     - [train] loss: 0.5643 (+/-0.177 ) - [valid] loss: 0.6114 (+/-0.1422) - [test] loss: 0.5603 (+/-0.1048)\n",
            "saving best model at iter 600\n",
            "Iter 600  - time: 7     - [train] loss: 0.3998 (+/-0.0837) - [valid] loss: 0.4685 (+/-0.0996) - [test] loss: 0.4864 (+/-0.1247)\n",
            "saving best model at iter 700\n",
            "Iter 700  - time: 8     - [train] loss: 0.4003 (+/-0.1095) - [valid] loss: 0.3958 (+/-0.0931) - [test] loss: 0.49   (+/-0.134 )\n",
            "saving best model at iter 800\n",
            "Iter 800  - time: 8     - [train] loss: 0.3822 (+/-0.0781) - [valid] loss: 0.3242 (+/-0.0573) - [test] loss: 0.4374 (+/-0.082 )\n",
            "Iter 900  - time: 8     - [train] loss: 0.3814 (+/-0.079 ) - [valid] loss: 0.3981 (+/-0.0928) - [test] loss: 0.3502 (+/-0.0735)\n",
            "Iter 1000 - time: 8     - [train] loss: 0.406  (+/-0.0845) - [valid] loss: 0.4031 (+/-0.0896) - [test] loss: 0.3863 (+/-0.0818)\n",
            "Iter 1100 - time: 8     - [train] loss: 0.3107 (+/-0.0722) - [valid] loss: 0.326  (+/-0.0825) - [test] loss: 0.2751 (+/-0.0726)\n",
            "saving best model at iter 1200\n",
            "Iter 1200 - time: 7     - [train] loss: 0.3186 (+/-0.0883) - [valid] loss: 0.2629 (+/-0.069 ) - [test] loss: 0.3698 (+/-0.0794)\n",
            "Iter 1300 - time: 8     - [train] loss: 0.2944 (+/-0.0686) - [valid] loss: 0.3101 (+/-0.0669) - [test] loss: 0.4098 (+/-0.1085)\n",
            "Iter 1400 - time: 8     - [train] loss: 0.3577 (+/-0.1041) - [valid] loss: 0.3201 (+/-0.0768) - [test] loss: 0.3433 (+/-0.0636)\n",
            "saving best model at iter 1500\n",
            "Iter 1500 - time: 7     - [train] loss: 0.2694 (+/-0.0581) - [valid] loss: 0.2457 (+/-0.0482) - [test] loss: 0.2731 (+/-0.0727)\n",
            "Iter 1600 - time: 8     - [train] loss: 0.3307 (+/-0.0787) - [valid] loss: 0.3215 (+/-0.0815) - [test] loss: 0.3344 (+/-0.0668)\n",
            "Iter 1700 - time: 8     - [train] loss: 0.3593 (+/-0.1039) - [valid] loss: 0.315  (+/-0.0788) - [test] loss: 0.3551 (+/-0.0984)\n",
            "Iter 1800 - time: 7     - [train] loss: 0.3177 (+/-0.062 ) - [valid] loss: 0.32   (+/-0.0754) - [test] loss: 0.3117 (+/-0.065 )\n",
            "Iter 1900 - time: 8     - [train] loss: 0.2699 (+/-0.0922) - [valid] loss: 0.2927 (+/-0.0831) - [test] loss: 0.2734 (+/-0.0709)\n",
            "Iter 2000 - time: 8     - [train] loss: 0.2806 (+/-0.065 ) - [valid] loss: 0.3073 (+/-0.063 ) - [test] loss: 0.3187 (+/-0.0728)\n",
            "Iter 2100 - time: 8     - [train] loss: 0.3001 (+/-0.0748) - [valid] loss: 0.3276 (+/-0.0698) - [test] loss: 0.3304 (+/-0.0767)\n",
            "Iter 2200 - time: 7     - [train] loss: 0.2709 (+/-0.059 ) - [valid] loss: 0.3084 (+/-0.0616) - [test] loss: 0.3326 (+/-0.091 )\n",
            "Iter 2300 - time: 8     - [train] loss: 0.2693 (+/-0.0643) - [valid] loss: 0.253  (+/-0.0557) - [test] loss: 0.2601 (+/-0.0581)\n",
            "Iter 2400 - time: 8     - [train] loss: 0.1886 (+/-0.0452) - [valid] loss: 0.2483 (+/-0.0678) - [test] loss: 0.2419 (+/-0.0685)\n",
            "Iter 2500 - time: 8     - [train] loss: 0.2838 (+/-0.0736) - [valid] loss: 0.2508 (+/-0.0758) - [test] loss: 0.29   (+/-0.0852)\n",
            "Iter 2600 - time: 8     - [train] loss: 0.2587 (+/-0.0596) - [valid] loss: 0.3842 (+/-0.0785) - [test] loss: 0.2533 (+/-0.0467)\n",
            "saving best model at iter 2700\n",
            "Iter 2700 - time: 7     - [train] loss: 0.2146 (+/-0.0507) - [valid] loss: 0.211  (+/-0.0536) - [test] loss: 0.2244 (+/-0.0678)\n",
            "Iter 2800 - time: 8     - [train] loss: 0.249  (+/-0.0608) - [valid] loss: 0.3043 (+/-0.0998) - [test] loss: 0.2501 (+/-0.0527)\n",
            "Iter 2900 - time: 7     - [train] loss: 0.2211 (+/-0.0476) - [valid] loss: 0.2748 (+/-0.0739) - [test] loss: 0.2545 (+/-0.0665)\n",
            "Iter 3000 - time: 8     - [train] loss: 0.3598 (+/-0.0692) - [valid] loss: 0.3534 (+/-0.0774) - [test] loss: 0.3165 (+/-0.0565)\n",
            "Iter 3100 - time: 8     - [train] loss: 0.29   (+/-0.0843) - [valid] loss: 0.2716 (+/-0.0911) - [test] loss: 0.2639 (+/-0.0561)\n",
            "Iter 3200 - time: 7     - [train] loss: 0.2483 (+/-0.0638) - [valid] loss: 0.2817 (+/-0.0592) - [test] loss: 0.2527 (+/-0.0622)\n",
            "Iter 3300 - time: 8     - [train] loss: 0.2132 (+/-0.0424) - [valid] loss: 0.2412 (+/-0.0486) - [test] loss: 0.2715 (+/-0.0566)\n",
            "Iter 3400 - time: 8     - [train] loss: 0.2506 (+/-0.0624) - [valid] loss: 0.2375 (+/-0.0582) - [test] loss: 0.2807 (+/-0.0855)\n",
            "Iter 3500 - time: 7     - [train] loss: 0.3632 (+/-0.0619) - [valid] loss: 0.2868 (+/-0.0456) - [test] loss: 0.3879 (+/-0.0717)\n",
            "Iter 3600 - time: 8     - [train] loss: 0.2591 (+/-0.0653) - [valid] loss: 0.3176 (+/-0.0726) - [test] loss: 0.2991 (+/-0.0663)\n",
            "Iter 3700 - time: 8     - [train] loss: 0.3423 (+/-0.0929) - [valid] loss: 0.2705 (+/-0.0536) - [test] loss: 0.3094 (+/-0.0544)\n",
            "Iter 3800 - time: 8     - [train] loss: 0.2843 (+/-0.0706) - [valid] loss: 0.2423 (+/-0.0559) - [test] loss: 0.3164 (+/-0.0873)\n",
            "Iter 3900 - time: 8     - [train] loss: 0.2697 (+/-0.0601) - [valid] loss: 0.3699 (+/-0.0993) - [test] loss: 0.245  (+/-0.0489)\n",
            "Iter 4000 - time: 8     - [train] loss: 0.3043 (+/-0.093 ) - [valid] loss: 0.2811 (+/-0.0678) - [test] loss: 0.2906 (+/-0.0844)\n",
            "Iter 4100 - time: 9     - [train] loss: 0.2521 (+/-0.0595) - [valid] loss: 0.3331 (+/-0.0934) - [test] loss: 0.2697 (+/-0.0601)\n",
            "Iter 4200 - time: 7     - [train] loss: 0.2873 (+/-0.071 ) - [valid] loss: 0.3042 (+/-0.0664) - [test] loss: 0.3078 (+/-0.1083)\n",
            "Iter 4300 - time: 8     - [train] loss: 0.227  (+/-0.0565) - [valid] loss: 0.2532 (+/-0.0615) - [test] loss: 0.2521 (+/-0.0549)\n",
            "Iter 4400 - time: 8     - [train] loss: 0.2744 (+/-0.0585) - [valid] loss: 0.2458 (+/-0.0596) - [test] loss: 0.2861 (+/-0.0602)\n",
            "Iter 4500 - time: 7     - [train] loss: 0.2246 (+/-0.0551) - [valid] loss: 0.2373 (+/-0.0635) - [test] loss: 0.2775 (+/-0.0777)\n",
            "Iter 4600 - time: 8     - [train] loss: 0.256  (+/-0.0578) - [valid] loss: 0.2524 (+/-0.0545) - [test] loss: 0.2955 (+/-0.0718)\n",
            "Iter 4700 - time: 8     - [train] loss: 0.3162 (+/-0.0676) - [valid] loss: 0.292  (+/-0.0722) - [test] loss: 0.214  (+/-0.0615)\n",
            "Iter 4800 - time: 10    - [train] loss: 0.3345 (+/-0.0564) - [valid] loss: 0.4111 (+/-0.0915) - [test] loss: 0.3679 (+/-0.0725)\n",
            "Iter 4900 - time: 7     - [train] loss: 0.2719 (+/-0.046 ) - [valid] loss: 0.2581 (+/-0.0528) - [test] loss: 0.2749 (+/-0.0722)\n",
            "Iter 5000 - time: 8     - [train] loss: 0.3595 (+/-0.0817) - [valid] loss: 0.3094 (+/-0.0681) - [test] loss: 0.3294 (+/-0.0792)\n",
            "Iter 5100 - time: 8     - [train] loss: 0.3505 (+/-0.0819) - [valid] loss: 0.2968 (+/-0.0665) - [test] loss: 0.3044 (+/-0.069 )\n",
            "Iter 5200 - time: 7     - [train] loss: 0.296  (+/-0.0789) - [valid] loss: 0.3029 (+/-0.0709) - [test] loss: 0.2377 (+/-0.0765)\n",
            "Iter 5300 - time: 8     - [train] loss: 0.2942 (+/-0.0707) - [valid] loss: 0.2796 (+/-0.083 ) - [test] loss: 0.2473 (+/-0.0606)\n",
            "Iter 5400 - time: 9     - [train] loss: 0.2977 (+/-0.0565) - [valid] loss: 0.2334 (+/-0.0408) - [test] loss: 0.2914 (+/-0.0696)\n",
            "Iter 5500 - time: 7     - [train] loss: 0.3113 (+/-0.067 ) - [valid] loss: 0.2663 (+/-0.0529) - [test] loss: 0.3156 (+/-0.0762)\n",
            "Iter 5600 - time: 8     - [train] loss: 0.2783 (+/-0.0855) - [valid] loss: 0.293  (+/-0.0898) - [test] loss: 0.345  (+/-0.0793)\n",
            "Iter 5700 - time: 9     - [train] loss: 0.2029 (+/-0.0696) - [valid] loss: 0.2954 (+/-0.0871) - [test] loss: 0.1923 (+/-0.0459)\n",
            "Iter 5800 - time: 7     - [train] loss: 0.25   (+/-0.0608) - [valid] loss: 0.3132 (+/-0.1051) - [test] loss: 0.3155 (+/-0.0799)\n",
            "Iter 5900 - time: 8     - [train] loss: 0.3346 (+/-0.0646) - [valid] loss: 0.2956 (+/-0.0549) - [test] loss: 0.3348 (+/-0.0604)\n",
            "Iter 6000 - time: 8     - [train] loss: 0.3614 (+/-0.0719) - [valid] loss: 0.4301 (+/-0.0872) - [test] loss: 0.4328 (+/-0.1181)\n",
            "Iter 6100 - time: 7     - [train] loss: 0.26   (+/-0.0725) - [valid] loss: 0.2878 (+/-0.0739) - [test] loss: 0.3055 (+/-0.0773)\n",
            "Iter 6200 - time: 8     - [train] loss: 0.2748 (+/-0.055 ) - [valid] loss: 0.2342 (+/-0.0462) - [test] loss: 0.235  (+/-0.0528)\n",
            "Iter 6300 - time: 8     - [train] loss: 0.3142 (+/-0.0856) - [valid] loss: 0.2893 (+/-0.0659) - [test] loss: 0.2586 (+/-0.0627)\n",
            "Iter 6400 - time: 7     - [train] loss: 0.2293 (+/-0.0525) - [valid] loss: 0.2926 (+/-0.077 ) - [test] loss: 0.2859 (+/-0.069 )\n",
            "Iter 6500 - time: 7     - [train] loss: 0.2774 (+/-0.064 ) - [valid] loss: 0.2543 (+/-0.0642) - [test] loss: 0.2407 (+/-0.0579)\n",
            "Iter 6600 - time: 7     - [train] loss: 0.2818 (+/-0.0609) - [valid] loss: 0.2362 (+/-0.0663) - [test] loss: 0.2155 (+/-0.0553)\n",
            "Iter 6700 - time: 7     - [train] loss: 0.3509 (+/-0.0724) - [valid] loss: 0.2816 (+/-0.0597) - [test] loss: 0.3921 (+/-0.1003)\n",
            "Iter 6800 - time: 7     - [train] loss: 0.2836 (+/-0.0589) - [valid] loss: 0.3026 (+/-0.066 ) - [test] loss: 0.3019 (+/-0.0723)\n",
            "Iter 6900 - time: 7     - [train] loss: 0.2034 (+/-0.0393) - [valid] loss: 0.231  (+/-0.0664) - [test] loss: 0.2225 (+/-0.0527)\n",
            "Iter 7000 - time: 7     - [train] loss: 0.2429 (+/-0.0412) - [valid] loss: 0.2952 (+/-0.0831) - [test] loss: 0.2397 (+/-0.0478)\n",
            "saving best model at iter 7100\n",
            "Iter 7100 - time: 7     - [train] loss: 0.2279 (+/-0.0595) - [valid] loss: 0.1952 (+/-0.0427) - [test] loss: 0.1969 (+/-0.0518)\n",
            "Iter 7200 - time: 7     - [train] loss: 0.2754 (+/-0.0741) - [valid] loss: 0.2623 (+/-0.0673) - [test] loss: 0.2759 (+/-0.0775)\n",
            "Iter 7300 - time: 7     - [train] loss: 0.2602 (+/-0.0763) - [valid] loss: 0.2431 (+/-0.0683) - [test] loss: 0.2422 (+/-0.0542)\n",
            "Iter 7400 - time: 7     - [train] loss: 0.3048 (+/-0.0661) - [valid] loss: 0.2341 (+/-0.052 ) - [test] loss: 0.3166 (+/-0.0871)\n",
            "Iter 7500 - time: 8     - [train] loss: 0.2552 (+/-0.0734) - [valid] loss: 0.2195 (+/-0.0551) - [test] loss: 0.221  (+/-0.0544)\n",
            "Iter 7600 - time: 8     - [train] loss: 0.3049 (+/-0.072 ) - [valid] loss: 0.3011 (+/-0.0849) - [test] loss: 0.3482 (+/-0.0764)\n",
            "Iter 7700 - time: 8     - [train] loss: 0.3455 (+/-0.1038) - [valid] loss: 0.2679 (+/-0.0608) - [test] loss: 0.3089 (+/-0.1195)\n",
            "Iter 7800 - time: 7     - [train] loss: 0.2982 (+/-0.1115) - [valid] loss: 0.2498 (+/-0.064 ) - [test] loss: 0.271  (+/-0.0732)\n",
            "Iter 7900 - time: 7     - [train] loss: 0.2497 (+/-0.0657) - [valid] loss: 0.2706 (+/-0.0725) - [test] loss: 0.2397 (+/-0.049 )\n",
            "Iter 8000 - time: 7     - [train] loss: 0.3015 (+/-0.0654) - [valid] loss: 0.282  (+/-0.0475) - [test] loss: 0.3089 (+/-0.0811)\n",
            "Iter 8100 - time: 7     - [train] loss: 0.326  (+/-0.0758) - [valid] loss: 0.3413 (+/-0.1125) - [test] loss: 0.3719 (+/-0.0898)\n",
            "Iter 8200 - time: 7     - [train] loss: 0.2504 (+/-0.1054) - [valid] loss: 0.2676 (+/-0.0696) - [test] loss: 0.3191 (+/-0.0901)\n",
            "Iter 8300 - time: 7     - [train] loss: 0.2072 (+/-0.0411) - [valid] loss: 0.219  (+/-0.0486) - [test] loss: 0.2354 (+/-0.0753)\n",
            "Iter 8400 - time: 7     - [train] loss: 0.2864 (+/-0.0633) - [valid] loss: 0.3123 (+/-0.0631) - [test] loss: 0.2915 (+/-0.0615)\n",
            "Iter 8500 - time: 7     - [train] loss: 0.2215 (+/-0.045 ) - [valid] loss: 0.206  (+/-0.0484) - [test] loss: 0.26   (+/-0.0686)\n",
            "Iter 8600 - time: 7     - [train] loss: 0.2224 (+/-0.0495) - [valid] loss: 0.2904 (+/-0.077 ) - [test] loss: 0.2086 (+/-0.0464)\n",
            "Iter 8700 - time: 7     - [train] loss: 0.2714 (+/-0.0632) - [valid] loss: 0.2725 (+/-0.0603) - [test] loss: 0.2719 (+/-0.0505)\n",
            "Iter 8800 - time: 7     - [train] loss: 0.2161 (+/-0.0528) - [valid] loss: 0.2786 (+/-0.0937) - [test] loss: 0.263  (+/-0.0685)\n",
            "Iter 8900 - time: 7     - [train] loss: 0.2689 (+/-0.0668) - [valid] loss: 0.2454 (+/-0.0744) - [test] loss: 0.2885 (+/-0.079 )\n",
            "Iter 9000 - time: 7     - [train] loss: 0.2513 (+/-0.0577) - [valid] loss: 0.2234 (+/-0.0462) - [test] loss: 0.2685 (+/-0.0731)\n",
            "Iter 9100 - time: 7     - [train] loss: 0.2541 (+/-0.0599) - [valid] loss: 0.2728 (+/-0.0838) - [test] loss: 0.2836 (+/-0.0878)\n",
            "Iter 9200 - time: 7     - [train] loss: 0.2247 (+/-0.0439) - [valid] loss: 0.1981 (+/-0.0437) - [test] loss: 0.2392 (+/-0.0551)\n",
            "Iter 9300 - time: 7     - [train] loss: 0.2924 (+/-0.1008) - [valid] loss: 0.2247 (+/-0.061 ) - [test] loss: 0.2277 (+/-0.0542)\n",
            "saving best model at iter 9400\n",
            "Iter 9400 - time: 7     - [train] loss: 0.2307 (+/-0.0484) - [valid] loss: 0.1896 (+/-0.0342) - [test] loss: 0.3467 (+/-0.0862)\n",
            "Iter 9500 - time: 7     - [train] loss: 0.2918 (+/-0.0728) - [valid] loss: 0.2292 (+/-0.056 ) - [test] loss: 0.303  (+/-0.0809)\n",
            "Iter 9600 - time: 7     - [train] loss: 0.2347 (+/-0.0599) - [valid] loss: 0.1942 (+/-0.0424) - [test] loss: 0.2102 (+/-0.0432)\n",
            "Iter 9700 - time: 7     - [train] loss: 0.2222 (+/-0.0544) - [valid] loss: 0.2352 (+/-0.0788) - [test] loss: 0.3022 (+/-0.0785)\n",
            "Iter 9800 - time: 7     - [train] loss: 0.2132 (+/-0.0493) - [valid] loss: 0.2325 (+/-0.0626) - [test] loss: 0.3179 (+/-0.0835)\n",
            "Iter 9900 - time: 7     - [train] loss: 0.333  (+/-0.0954) - [valid] loss: 0.3376 (+/-0.0847) - [test] loss: 0.2888 (+/-0.0929)\n",
            "Iter 10000 - time: 7     - [train] loss: 0.1964 (+/-0.0439) - [valid] loss: 0.275  (+/-0.0682) - [test] loss: 0.2774 (+/-0.0816)\n",
            "Iter 10100 - time: 7     - [train] loss: 0.3076 (+/-0.0836) - [valid] loss: 0.3138 (+/-0.0936) - [test] loss: 0.2412 (+/-0.0584)\n",
            "Iter 10200 - time: 7     - [train] loss: 0.2428 (+/-0.0649) - [valid] loss: 0.1991 (+/-0.0407) - [test] loss: 0.2826 (+/-0.062 )\n",
            "Iter 10300 - time: 7     - [train] loss: 0.2902 (+/-0.0691) - [valid] loss: 0.3079 (+/-0.0874) - [test] loss: 0.2223 (+/-0.0373)\n",
            "Iter 10400 - time: 7     - [train] loss: 0.2862 (+/-0.079 ) - [valid] loss: 0.2177 (+/-0.0515) - [test] loss: 0.2037 (+/-0.0443)\n",
            "Iter 10500 - time: 7     - [train] loss: 0.2374 (+/-0.0479) - [valid] loss: 0.25   (+/-0.0529) - [test] loss: 0.2018 (+/-0.0436)\n",
            "Iter 10600 - time: 7     - [train] loss: 0.2047 (+/-0.0482) - [valid] loss: 0.1982 (+/-0.0528) - [test] loss: 0.2116 (+/-0.0508)\n",
            "Iter 10700 - time: 7     - [train] loss: 0.2919 (+/-0.0747) - [valid] loss: 0.3076 (+/-0.0808) - [test] loss: 0.3115 (+/-0.0963)\n",
            "Iter 10800 - time: 7     - [train] loss: 0.1951 (+/-0.0438) - [valid] loss: 0.2354 (+/-0.0528) - [test] loss: 0.2913 (+/-0.0617)\n",
            "Iter 10900 - time: 7     - [train] loss: 0.2354 (+/-0.0619) - [valid] loss: 0.2019 (+/-0.0451) - [test] loss: 0.247  (+/-0.0591)\n",
            "Iter 11000 - time: 7     - [train] loss: 0.2492 (+/-0.0413) - [valid] loss: 0.284  (+/-0.0605) - [test] loss: 0.2169 (+/-0.0472)\n",
            "Iter 11100 - time: 7     - [train] loss: 0.2877 (+/-0.0733) - [valid] loss: 0.3024 (+/-0.0722) - [test] loss: 0.2853 (+/-0.0725)\n",
            "Iter 11200 - time: 7     - [train] loss: 0.2691 (+/-0.1085) - [valid] loss: 0.3479 (+/-0.0901) - [test] loss: 0.2678 (+/-0.0564)\n",
            "Iter 11300 - time: 7     - [train] loss: 0.2123 (+/-0.0374) - [valid] loss: 0.2052 (+/-0.0458) - [test] loss: 0.2246 (+/-0.06  )\n",
            "Iter 11400 - time: 7     - [train] loss: 0.2924 (+/-0.0685) - [valid] loss: 0.1908 (+/-0.0519) - [test] loss: 0.2034 (+/-0.0441)\n",
            "Iter 11500 - time: 7     - [train] loss: 0.3243 (+/-0.0723) - [valid] loss: 0.3014 (+/-0.0582) - [test] loss: 0.2771 (+/-0.0583)\n",
            "Iter 11600 - time: 8     - [train] loss: 0.276  (+/-0.0706) - [valid] loss: 0.2122 (+/-0.0461) - [test] loss: 0.2525 (+/-0.0745)\n",
            "Iter 11700 - time: 7     - [train] loss: 0.2439 (+/-0.0511) - [valid] loss: 0.2469 (+/-0.0517) - [test] loss: 0.2137 (+/-0.0408)\n",
            "Iter 11800 - time: 7     - [train] loss: 0.2477 (+/-0.0553) - [valid] loss: 0.2583 (+/-0.0668) - [test] loss: 0.2361 (+/-0.0445)\n",
            "Iter 11900 - time: 7     - [train] loss: 0.3216 (+/-0.0795) - [valid] loss: 0.2962 (+/-0.0671) - [test] loss: 0.2769 (+/-0.0552)\n",
            "Iter 12000 - time: 7     - [train] loss: 0.2255 (+/-0.0664) - [valid] loss: 0.2381 (+/-0.0573) - [test] loss: 0.1986 (+/-0.052 )\n",
            "Iter 12100 - time: 7     - [train] loss: 0.2784 (+/-0.0962) - [valid] loss: 0.2736 (+/-0.0877) - [test] loss: 0.2648 (+/-0.0702)\n",
            "Iter 12200 - time: 7     - [train] loss: 0.2592 (+/-0.0571) - [valid] loss: 0.2921 (+/-0.0703) - [test] loss: 0.2856 (+/-0.0711)\n",
            "Iter 12300 - time: 7     - [train] loss: 0.2983 (+/-0.0891) - [valid] loss: 0.2578 (+/-0.0637) - [test] loss: 0.2211 (+/-0.0434)\n",
            "Iter 12400 - time: 7     - [train] loss: 0.238  (+/-0.0565) - [valid] loss: 0.2297 (+/-0.0755) - [test] loss: 0.1914 (+/-0.0465)\n",
            "Iter 12500 - time: 7     - [train] loss: 0.2788 (+/-0.0845) - [valid] loss: 0.2317 (+/-0.0501) - [test] loss: 0.2417 (+/-0.0579)\n",
            "Iter 12600 - time: 7     - [train] loss: 0.2504 (+/-0.0628) - [valid] loss: 0.2082 (+/-0.0517) - [test] loss: 0.2983 (+/-0.0739)\n",
            "Iter 12700 - time: 7     - [train] loss: 0.2355 (+/-0.0524) - [valid] loss: 0.2605 (+/-0.0483) - [test] loss: 0.2584 (+/-0.0683)\n",
            "Iter 12800 - time: 7     - [train] loss: 0.2724 (+/-0.0671) - [valid] loss: 0.2297 (+/-0.0593) - [test] loss: 0.2242 (+/-0.0542)\n",
            "Iter 12900 - time: 7     - [train] loss: 0.2432 (+/-0.0442) - [valid] loss: 0.2908 (+/-0.0683) - [test] loss: 0.245  (+/-0.0411)\n",
            "Iter 13000 - time: 7     - [train] loss: 0.2602 (+/-0.0709) - [valid] loss: 0.2476 (+/-0.0964) - [test] loss: 0.2582 (+/-0.0638)\n",
            "Iter 13100 - time: 7     - [train] loss: 0.2884 (+/-0.0873) - [valid] loss: 0.2939 (+/-0.0708) - [test] loss: 0.2275 (+/-0.0502)\n",
            "Iter 13200 - time: 7     - [train] loss: 0.1819 (+/-0.0349) - [valid] loss: 0.286  (+/-0.0699) - [test] loss: 0.2468 (+/-0.0614)\n",
            "Iter 13300 - time: 7     - [train] loss: 0.2367 (+/-0.0446) - [valid] loss: 0.2514 (+/-0.0515) - [test] loss: 0.2136 (+/-0.0431)\n",
            "Iter 13400 - time: 7     - [train] loss: 0.2636 (+/-0.0799) - [valid] loss: 0.3144 (+/-0.0795) - [test] loss: 0.2317 (+/-0.0561)\n",
            "Iter 13500 - time: 7     - [train] loss: 0.2473 (+/-0.0647) - [valid] loss: 0.2351 (+/-0.0475) - [test] loss: 0.2205 (+/-0.0632)\n",
            "Iter 13600 - time: 7     - [train] loss: 0.2267 (+/-0.0653) - [valid] loss: 0.2049 (+/-0.0496) - [test] loss: 0.2302 (+/-0.05  )\n",
            "Iter 13700 - time: 7     - [train] loss: 0.2322 (+/-0.0735) - [valid] loss: 0.2219 (+/-0.0459) - [test] loss: 0.2885 (+/-0.1132)\n",
            "Iter 13800 - time: 7     - [train] loss: 0.2633 (+/-0.0731) - [valid] loss: 0.1931 (+/-0.0444) - [test] loss: 0.2527 (+/-0.0474)\n",
            "Iter 13900 - time: 7     - [train] loss: 0.3005 (+/-0.0654) - [valid] loss: 0.2439 (+/-0.0483) - [test] loss: 0.2654 (+/-0.0555)\n",
            "Iter 14000 - time: 7     - [train] loss: 0.2302 (+/-0.0726) - [valid] loss: 0.2306 (+/-0.0497) - [test] loss: 0.2553 (+/-0.0583)\n",
            "Iter 14100 - time: 7     - [train] loss: 0.2181 (+/-0.0621) - [valid] loss: 0.2191 (+/-0.0588) - [test] loss: 0.2171 (+/-0.0586)\n",
            "Iter 14200 - time: 7     - [train] loss: 0.2596 (+/-0.0546) - [valid] loss: 0.2611 (+/-0.0763) - [test] loss: 0.2081 (+/-0.0482)\n",
            "Iter 14300 - time: 7     - [train] loss: 0.3152 (+/-0.0881) - [valid] loss: 0.3168 (+/-0.0878) - [test] loss: 0.2459 (+/-0.0511)\n",
            "Iter 14400 - time: 7     - [train] loss: 0.2563 (+/-0.0823) - [valid] loss: 0.3073 (+/-0.0745) - [test] loss: 0.2024 (+/-0.0504)\n",
            "Iter 14500 - time: 7     - [train] loss: 0.2119 (+/-0.0429) - [valid] loss: 0.2195 (+/-0.0616) - [test] loss: 0.2181 (+/-0.0567)\n",
            "Iter 14600 - time: 7     - [train] loss: 0.3186 (+/-0.0724) - [valid] loss: 0.257  (+/-0.0577) - [test] loss: 0.2795 (+/-0.0707)\n",
            "Iter 14700 - time: 7     - [train] loss: 0.3494 (+/-0.0692) - [valid] loss: 0.2742 (+/-0.0698) - [test] loss: 0.2617 (+/-0.0513)\n",
            "Iter 14800 - time: 7     - [train] loss: 0.2985 (+/-0.0691) - [valid] loss: 0.2547 (+/-0.064 ) - [test] loss: 0.2545 (+/-0.0485)\n",
            "Iter 14900 - time: 7     - [train] loss: 0.2668 (+/-0.1056) - [valid] loss: 0.2374 (+/-0.061 ) - [test] loss: 0.2186 (+/-0.0442)\n",
            "Iter 15000 - time: 7     - [train] loss: 0.2829 (+/-0.0461) - [valid] loss: 0.3042 (+/-0.0865) - [test] loss: 0.1767 (+/-0.0301)\n",
            "Iter 15100 - time: 7     - [train] loss: 0.2104 (+/-0.0503) - [valid] loss: 0.3699 (+/-0.122 ) - [test] loss: 0.2766 (+/-0.0683)\n",
            "Iter 15200 - time: 7     - [train] loss: 0.2615 (+/-0.0848) - [valid] loss: 0.2523 (+/-0.0637) - [test] loss: 0.2119 (+/-0.0433)\n",
            "Iter 15300 - time: 8     - [train] loss: 0.2321 (+/-0.0646) - [valid] loss: 0.2548 (+/-0.0812) - [test] loss: 0.1893 (+/-0.0589)\n",
            "Iter 15400 - time: 8     - [train] loss: 0.2484 (+/-0.0461) - [valid] loss: 0.2287 (+/-0.0409) - [test] loss: 0.2752 (+/-0.0906)\n",
            "Iter 15500 - time: 8     - [train] loss: 0.2707 (+/-0.068 ) - [valid] loss: 0.2528 (+/-0.0675) - [test] loss: 0.3234 (+/-0.084 )\n",
            "Iter 15600 - time: 8     - [train] loss: 0.1996 (+/-0.0522) - [valid] loss: 0.2374 (+/-0.0576) - [test] loss: 0.251  (+/-0.0693)\n",
            "Iter 15700 - time: 8     - [train] loss: 0.2559 (+/-0.0598) - [valid] loss: 0.2223 (+/-0.0497) - [test] loss: 0.2421 (+/-0.0708)\n",
            "Iter 15800 - time: 8     - [train] loss: 0.2398 (+/-0.0766) - [valid] loss: 0.2723 (+/-0.0875) - [test] loss: 0.2871 (+/-0.093 )\n",
            "Iter 15900 - time: 7     - [train] loss: 0.2557 (+/-0.0673) - [valid] loss: 0.2242 (+/-0.0595) - [test] loss: 0.268  (+/-0.0686)\n",
            "Iter 16000 - time: 8     - [train] loss: 0.2426 (+/-0.0521) - [valid] loss: 0.2808 (+/-0.0632) - [test] loss: 0.261  (+/-0.0593)\n",
            "Iter 16100 - time: 7     - [train] loss: 0.246  (+/-0.0574) - [valid] loss: 0.2603 (+/-0.0834) - [test] loss: 0.2864 (+/-0.0505)\n",
            "Iter 16200 - time: 7     - [train] loss: 0.2175 (+/-0.0451) - [valid] loss: 0.2461 (+/-0.0589) - [test] loss: 0.2461 (+/-0.0673)\n",
            "Iter 16300 - time: 7     - [train] loss: 0.2652 (+/-0.1002) - [valid] loss: 0.2321 (+/-0.0708) - [test] loss: 0.2866 (+/-0.0662)\n",
            "Iter 16400 - time: 7     - [train] loss: 0.2494 (+/-0.0826) - [valid] loss: 0.2254 (+/-0.0824) - [test] loss: 0.2226 (+/-0.0479)\n",
            "Iter 16500 - time: 7     - [train] loss: 0.2203 (+/-0.0505) - [valid] loss: 0.2356 (+/-0.0524) - [test] loss: 0.3125 (+/-0.081 )\n",
            "Iter 16600 - time: 7     - [train] loss: 0.2271 (+/-0.0416) - [valid] loss: 0.2685 (+/-0.0541) - [test] loss: 0.2824 (+/-0.0598)\n",
            "Iter 16700 - time: 7     - [train] loss: 0.206  (+/-0.051 ) - [valid] loss: 0.2189 (+/-0.0497) - [test] loss: 0.1994 (+/-0.0588)\n",
            "Iter 16800 - time: 7     - [train] loss: 0.2556 (+/-0.0625) - [valid] loss: 0.2029 (+/-0.0426) - [test] loss: 0.2389 (+/-0.0556)\n",
            "Iter 16900 - time: 7     - [train] loss: 0.2707 (+/-0.0609) - [valid] loss: 0.2027 (+/-0.0503) - [test] loss: 0.2699 (+/-0.0711)\n",
            "Iter 17000 - time: 7     - [train] loss: 0.2248 (+/-0.063 ) - [valid] loss: 0.2387 (+/-0.0616) - [test] loss: 0.2387 (+/-0.0554)\n",
            "Iter 17100 - time: 7     - [train] loss: 0.2211 (+/-0.0499) - [valid] loss: 0.2164 (+/-0.0628) - [test] loss: 0.1932 (+/-0.04  )\n",
            "Iter 17200 - time: 7     - [train] loss: 0.2743 (+/-0.0628) - [valid] loss: 0.2614 (+/-0.0591) - [test] loss: 0.3374 (+/-0.0924)\n",
            "Iter 17300 - time: 7     - [train] loss: 0.2373 (+/-0.0513) - [valid] loss: 0.2726 (+/-0.0794) - [test] loss: 0.2211 (+/-0.0557)\n",
            "Iter 17400 - time: 7     - [train] loss: 0.2401 (+/-0.0503) - [valid] loss: 0.2426 (+/-0.0574) - [test] loss: 0.277  (+/-0.0671)\n",
            "saving best model at iter 17500\n",
            "Iter 17500 - time: 7     - [train] loss: 0.1814 (+/-0.0408) - [valid] loss: 0.177  (+/-0.0389) - [test] loss: 0.2363 (+/-0.064 )\n",
            "Iter 17600 - time: 7     - [train] loss: 0.1956 (+/-0.0468) - [valid] loss: 0.2601 (+/-0.0905) - [test] loss: 0.295  (+/-0.1069)\n",
            "Iter 17700 - time: 7     - [train] loss: 0.1932 (+/-0.0314) - [valid] loss: 0.2134 (+/-0.0457) - [test] loss: 0.2289 (+/-0.0489)\n",
            "Iter 17800 - time: 7     - [train] loss: 0.254  (+/-0.0537) - [valid] loss: 0.3429 (+/-0.0622) - [test] loss: 0.2693 (+/-0.0502)\n",
            "Iter 17900 - time: 7     - [train] loss: 0.2707 (+/-0.0532) - [valid] loss: 0.2051 (+/-0.0487) - [test] loss: 0.2277 (+/-0.0573)\n",
            "Iter 18000 - time: 7     - [train] loss: 0.2723 (+/-0.0856) - [valid] loss: 0.2978 (+/-0.0968) - [test] loss: 0.2127 (+/-0.0431)\n",
            "Iter 18100 - time: 7     - [train] loss: 0.2423 (+/-0.0582) - [valid] loss: 0.2808 (+/-0.0763) - [test] loss: 0.2485 (+/-0.0672)\n",
            "Iter 18200 - time: 7     - [train] loss: 0.219  (+/-0.0544) - [valid] loss: 0.2492 (+/-0.0552) - [test] loss: 0.2449 (+/-0.0551)\n",
            "Iter 18300 - time: 7     - [train] loss: 0.2303 (+/-0.0635) - [valid] loss: 0.1821 (+/-0.0384) - [test] loss: 0.1837 (+/-0.0482)\n",
            "saving best model at iter 18400\n",
            "Iter 18400 - time: 7     - [train] loss: 0.2644 (+/-0.0654) - [valid] loss: 0.1757 (+/-0.0376) - [test] loss: 0.2518 (+/-0.0697)\n",
            "Iter 18500 - time: 7     - [train] loss: 0.2209 (+/-0.0457) - [valid] loss: 0.247  (+/-0.0598) - [test] loss: 0.2441 (+/-0.0651)\n",
            "Iter 18600 - time: 8     - [train] loss: 0.2064 (+/-0.0675) - [valid] loss: 0.2178 (+/-0.0507) - [test] loss: 0.2011 (+/-0.0486)\n",
            "Iter 18700 - time: 8     - [train] loss: 0.1868 (+/-0.0497) - [valid] loss: 0.2609 (+/-0.0596) - [test] loss: 0.2144 (+/-0.0476)\n",
            "Iter 18800 - time: 8     - [train] loss: 0.2459 (+/-0.0484) - [valid] loss: 0.1978 (+/-0.0448) - [test] loss: 0.2177 (+/-0.0504)\n",
            "Iter 18900 - time: 8     - [train] loss: 0.2344 (+/-0.0475) - [valid] loss: 0.2291 (+/-0.0604) - [test] loss: 0.2008 (+/-0.0382)\n",
            "Iter 19000 - time: 8     - [train] loss: 0.2696 (+/-0.0704) - [valid] loss: 0.2853 (+/-0.0672) - [test] loss: 0.293  (+/-0.0701)\n",
            "Iter 19100 - time: 8     - [train] loss: 0.222  (+/-0.0451) - [valid] loss: 0.2252 (+/-0.0483) - [test] loss: 0.2247 (+/-0.0444)\n",
            "Iter 19200 - time: 8     - [train] loss: 0.2244 (+/-0.0589) - [valid] loss: 0.2106 (+/-0.0448) - [test] loss: 0.2228 (+/-0.0409)\n",
            "Iter 19300 - time: 8     - [train] loss: 0.2906 (+/-0.089 ) - [valid] loss: 0.1957 (+/-0.0534) - [test] loss: 0.2264 (+/-0.0489)\n",
            "Iter 19400 - time: 8     - [train] loss: 0.2777 (+/-0.0621) - [valid] loss: 0.189  (+/-0.0596) - [test] loss: 0.2125 (+/-0.0567)\n",
            "Iter 19500 - time: 8     - [train] loss: 0.201  (+/-0.0542) - [valid] loss: 0.2619 (+/-0.0649) - [test] loss: 0.2104 (+/-0.0453)\n",
            "Iter 19600 - time: 8     - [train] loss: 0.2386 (+/-0.0552) - [valid] loss: 0.2348 (+/-0.0516) - [test] loss: 0.291  (+/-0.0672)\n",
            "Iter 19700 - time: 8     - [train] loss: 0.2136 (+/-0.0525) - [valid] loss: 0.2567 (+/-0.0572) - [test] loss: 0.2046 (+/-0.0604)\n",
            "Iter 19800 - time: 7     - [train] loss: 0.2396 (+/-0.056 ) - [valid] loss: 0.2189 (+/-0.0421) - [test] loss: 0.2266 (+/-0.0503)\n",
            "Iter 19900 - time: 7     - [train] loss: 0.2685 (+/-0.056 ) - [valid] loss: 0.253  (+/-0.0568) - [test] loss: 0.234  (+/-0.065 )\n",
            "Iter 20000 - time: 7     - [train] loss: 0.2666 (+/-0.0696) - [valid] loss: 0.2493 (+/-0.0637) - [test] loss: 0.2629 (+/-0.0712)\n",
            "Iter 20100 - time: 7     - [train] loss: 0.2516 (+/-0.0751) - [valid] loss: 0.2512 (+/-0.0673) - [test] loss: 0.3153 (+/-0.1044)\n",
            "Iter 20200 - time: 7     - [train] loss: 0.2136 (+/-0.044 ) - [valid] loss: 0.2383 (+/-0.0639) - [test] loss: 0.1919 (+/-0.04  )\n",
            "Iter 20300 - time: 7     - [train] loss: 0.237  (+/-0.0614) - [valid] loss: 0.2599 (+/-0.0627) - [test] loss: 0.2044 (+/-0.0392)\n",
            "Iter 20400 - time: 7     - [train] loss: 0.2666 (+/-0.063 ) - [valid] loss: 0.2433 (+/-0.0529) - [test] loss: 0.2536 (+/-0.0591)\n",
            "Iter 20500 - time: 7     - [train] loss: 0.2817 (+/-0.0648) - [valid] loss: 0.2486 (+/-0.0527) - [test] loss: 0.34   (+/-0.0965)\n",
            "Iter 20600 - time: 7     - [train] loss: 0.2868 (+/-0.0654) - [valid] loss: 0.209  (+/-0.049 ) - [test] loss: 0.2129 (+/-0.0512)\n",
            "Iter 20700 - time: 7     - [train] loss: 0.2401 (+/-0.0612) - [valid] loss: 0.2645 (+/-0.0629) - [test] loss: 0.2097 (+/-0.0408)\n",
            "Iter 20800 - time: 7     - [train] loss: 0.25   (+/-0.0628) - [valid] loss: 0.2087 (+/-0.0428) - [test] loss: 0.2336 (+/-0.055 )\n",
            "Iter 20900 - time: 7     - [train] loss: 0.2155 (+/-0.0555) - [valid] loss: 0.2109 (+/-0.0463) - [test] loss: 0.1755 (+/-0.0438)\n",
            "Iter 21000 - time: 7     - [train] loss: 0.3023 (+/-0.1113) - [valid] loss: 0.3001 (+/-0.0901) - [test] loss: 0.2107 (+/-0.0494)\n",
            "Iter 21100 - time: 7     - [train] loss: 0.2417 (+/-0.0534) - [valid] loss: 0.2548 (+/-0.0591) - [test] loss: 0.2448 (+/-0.0571)\n",
            "Iter 21200 - time: 7     - [train] loss: 0.2191 (+/-0.0538) - [valid] loss: 0.2349 (+/-0.074 ) - [test] loss: 0.2637 (+/-0.0714)\n",
            "Iter 21300 - time: 7     - [train] loss: 0.2687 (+/-0.0727) - [valid] loss: 0.23   (+/-0.0651) - [test] loss: 0.2259 (+/-0.0503)\n",
            "Iter 21400 - time: 7     - [train] loss: 0.2123 (+/-0.0614) - [valid] loss: 0.1838 (+/-0.0422) - [test] loss: 0.1991 (+/-0.0385)\n",
            "Iter 21500 - time: 7     - [train] loss: 0.2395 (+/-0.0504) - [valid] loss: 0.3015 (+/-0.0872) - [test] loss: 0.1812 (+/-0.0402)\n",
            "Iter 21600 - time: 7     - [train] loss: 0.3069 (+/-0.0858) - [valid] loss: 0.2585 (+/-0.0748) - [test] loss: 0.2318 (+/-0.0672)\n",
            "Iter 21700 - time: 7     - [train] loss: 0.1987 (+/-0.0552) - [valid] loss: 0.2347 (+/-0.0598) - [test] loss: 0.188  (+/-0.045 )\n",
            "Iter 21800 - time: 7     - [train] loss: 0.2492 (+/-0.057 ) - [valid] loss: 0.2559 (+/-0.0665) - [test] loss: 0.1819 (+/-0.0384)\n",
            "Iter 21900 - time: 7     - [train] loss: 0.1814 (+/-0.0337) - [valid] loss: 0.2557 (+/-0.0605) - [test] loss: 0.2395 (+/-0.051 )\n",
            "Iter 22000 - time: 7     - [train] loss: 0.2158 (+/-0.0531) - [valid] loss: 0.2523 (+/-0.0568) - [test] loss: 0.2578 (+/-0.0621)\n",
            "Iter 22100 - time: 7     - [train] loss: 0.1682 (+/-0.0434) - [valid] loss: 0.2699 (+/-0.0663) - [test] loss: 0.2231 (+/-0.0479)\n",
            "Iter 22200 - time: 7     - [train] loss: 0.2425 (+/-0.0694) - [valid] loss: 0.2328 (+/-0.0623) - [test] loss: 0.2121 (+/-0.051 )\n",
            "Iter 22300 - time: 7     - [train] loss: 0.234  (+/-0.0534) - [valid] loss: 0.2046 (+/-0.0397) - [test] loss: 0.2629 (+/-0.066 )\n",
            "Iter 22400 - time: 7     - [train] loss: 0.167  (+/-0.0375) - [valid] loss: 0.1969 (+/-0.0393) - [test] loss: 0.2743 (+/-0.0945)\n",
            "Iter 22500 - time: 7     - [train] loss: 0.2442 (+/-0.0694) - [valid] loss: 0.2325 (+/-0.058 ) - [test] loss: 0.2647 (+/-0.0683)\n",
            "Iter 22600 - time: 7     - [train] loss: 0.238  (+/-0.0744) - [valid] loss: 0.2228 (+/-0.0537) - [test] loss: 0.2121 (+/-0.0416)\n",
            "Iter 22700 - time: 7     - [train] loss: 0.2882 (+/-0.0698) - [valid] loss: 0.3417 (+/-0.0863) - [test] loss: 0.2897 (+/-0.0685)\n",
            "Iter 22800 - time: 7     - [train] loss: 0.1993 (+/-0.0459) - [valid] loss: 0.2317 (+/-0.0672) - [test] loss: 0.1262 (+/-0.0306)\n",
            "Iter 22900 - time: 7     - [train] loss: 0.1904 (+/-0.0393) - [valid] loss: 0.2525 (+/-0.0592) - [test] loss: 0.1989 (+/-0.0491)\n",
            "Iter 23000 - time: 7     - [train] loss: 0.2356 (+/-0.0596) - [valid] loss: 0.2906 (+/-0.0778) - [test] loss: 0.3052 (+/-0.0811)\n",
            "Iter 23100 - time: 7     - [train] loss: 0.2434 (+/-0.0544) - [valid] loss: 0.2316 (+/-0.0662) - [test] loss: 0.2347 (+/-0.0502)\n",
            "saving best model at iter 23200\n",
            "Iter 23200 - time: 7     - [train] loss: 0.2056 (+/-0.0619) - [valid] loss: 0.1502 (+/-0.0307) - [test] loss: 0.2697 (+/-0.0886)\n",
            "Iter 23300 - time: 7     - [train] loss: 0.2298 (+/-0.0706) - [valid] loss: 0.2512 (+/-0.0789) - [test] loss: 0.1941 (+/-0.0508)\n",
            "Iter 23400 - time: 7     - [train] loss: 0.2868 (+/-0.0852) - [valid] loss: 0.1793 (+/-0.0381) - [test] loss: 0.2971 (+/-0.0884)\n",
            "Iter 23500 - time: 7     - [train] loss: 0.2088 (+/-0.0388) - [valid] loss: 0.2078 (+/-0.0437) - [test] loss: 0.2523 (+/-0.0522)\n",
            "Iter 23600 - time: 8     - [train] loss: 0.1907 (+/-0.0332) - [valid] loss: 0.2262 (+/-0.0601) - [test] loss: 0.1941 (+/-0.0456)\n",
            "Iter 23700 - time: 7     - [train] loss: 0.2411 (+/-0.0671) - [valid] loss: 0.199  (+/-0.0524) - [test] loss: 0.184  (+/-0.058 )\n",
            "Iter 23800 - time: 7     - [train] loss: 0.2515 (+/-0.0496) - [valid] loss: 0.211  (+/-0.0523) - [test] loss: 0.1988 (+/-0.0573)\n",
            "Iter 23900 - time: 7     - [train] loss: 0.1695 (+/-0.0376) - [valid] loss: 0.2508 (+/-0.0682) - [test] loss: 0.1991 (+/-0.0595)\n",
            "Iter 24000 - time: 7     - [train] loss: 0.2034 (+/-0.0558) - [valid] loss: 0.1719 (+/-0.041 ) - [test] loss: 0.2373 (+/-0.078 )\n",
            "Iter 24100 - time: 7     - [train] loss: 0.22   (+/-0.0465) - [valid] loss: 0.2473 (+/-0.0489) - [test] loss: 0.242  (+/-0.0619)\n",
            "Iter 24200 - time: 7     - [train] loss: 0.2217 (+/-0.0546) - [valid] loss: 0.2091 (+/-0.0619) - [test] loss: 0.2385 (+/-0.0533)\n",
            "Iter 24300 - time: 7     - [train] loss: 0.2157 (+/-0.0478) - [valid] loss: 0.2015 (+/-0.0453) - [test] loss: 0.1954 (+/-0.0379)\n",
            "Iter 24400 - time: 7     - [train] loss: 0.3089 (+/-0.0815) - [valid] loss: 0.302  (+/-0.0687) - [test] loss: 0.2937 (+/-0.0703)\n",
            "Iter 24500 - time: 7     - [train] loss: 0.291  (+/-0.0955) - [valid] loss: 0.2388 (+/-0.0628) - [test] loss: 0.1736 (+/-0.0402)\n",
            "Iter 24600 - time: 7     - [train] loss: 0.2751 (+/-0.0775) - [valid] loss: 0.3149 (+/-0.0555) - [test] loss: 0.2775 (+/-0.0767)\n",
            "Iter 24700 - time: 7     - [train] loss: 0.2496 (+/-0.0547) - [valid] loss: 0.2381 (+/-0.0823) - [test] loss: 0.2738 (+/-0.0773)\n",
            "Iter 24800 - time: 7     - [train] loss: 0.255  (+/-0.0897) - [valid] loss: 0.2015 (+/-0.0603) - [test] loss: 0.1631 (+/-0.0368)\n",
            "Iter 24900 - time: 7     - [train] loss: 0.2554 (+/-0.0672) - [valid] loss: 0.2488 (+/-0.0642) - [test] loss: 0.2741 (+/-0.0891)\n",
            "Iter 25000 - time: 7     - [train] loss: 0.2681 (+/-0.079 ) - [valid] loss: 0.3041 (+/-0.0837) - [test] loss: 0.2234 (+/-0.0603)\n",
            "Iter 25100 - time: 7     - [train] loss: 0.2597 (+/-0.0804) - [valid] loss: 0.1915 (+/-0.0465) - [test] loss: 0.2011 (+/-0.0463)\n",
            "Iter 25200 - time: 7     - [train] loss: 0.2608 (+/-0.0783) - [valid] loss: 0.2998 (+/-0.0678) - [test] loss: 0.2348 (+/-0.0666)\n",
            "Iter 25300 - time: 7     - [train] loss: 0.2002 (+/-0.0595) - [valid] loss: 0.1982 (+/-0.0536) - [test] loss: 0.2447 (+/-0.0868)\n",
            "Iter 25400 - time: 7     - [train] loss: 0.2391 (+/-0.068 ) - [valid] loss: 0.2081 (+/-0.0553) - [test] loss: 0.2091 (+/-0.0499)\n",
            "Iter 25500 - time: 7     - [train] loss: 0.1989 (+/-0.0463) - [valid] loss: 0.1947 (+/-0.0448) - [test] loss: 0.1949 (+/-0.0512)\n",
            "Iter 25600 - time: 7     - [train] loss: 0.236  (+/-0.0598) - [valid] loss: 0.1792 (+/-0.0389) - [test] loss: 0.1791 (+/-0.0361)\n",
            "Iter 25700 - time: 7     - [train] loss: 0.2676 (+/-0.0735) - [valid] loss: 0.3298 (+/-0.0931) - [test] loss: 0.2313 (+/-0.053 )\n",
            "Iter 25800 - time: 7     - [train] loss: 0.236  (+/-0.0701) - [valid] loss: 0.2198 (+/-0.0562) - [test] loss: 0.2409 (+/-0.0666)\n",
            "Iter 25900 - time: 7     - [train] loss: 0.2482 (+/-0.0557) - [valid] loss: 0.2466 (+/-0.0595) - [test] loss: 0.2833 (+/-0.0877)\n",
            "Iter 26000 - time: 7     - [train] loss: 0.2842 (+/-0.0854) - [valid] loss: 0.2786 (+/-0.0527) - [test] loss: 0.2533 (+/-0.0422)\n",
            "Iter 26100 - time: 7     - [train] loss: 0.2048 (+/-0.0538) - [valid] loss: 0.2279 (+/-0.0475) - [test] loss: 0.1879 (+/-0.0389)\n",
            "Iter 26200 - time: 7     - [train] loss: 0.2084 (+/-0.0416) - [valid] loss: 0.2568 (+/-0.0575) - [test] loss: 0.2355 (+/-0.0531)\n",
            "Iter 26300 - time: 7     - [train] loss: 0.2086 (+/-0.0493) - [valid] loss: 0.3063 (+/-0.0823) - [test] loss: 0.1979 (+/-0.0608)\n",
            "Iter 26400 - time: 7     - [train] loss: 0.2255 (+/-0.0523) - [valid] loss: 0.1656 (+/-0.0435) - [test] loss: 0.1771 (+/-0.0433)\n",
            "Iter 26500 - time: 7     - [train] loss: 0.2887 (+/-0.0649) - [valid] loss: 0.2908 (+/-0.08  ) - [test] loss: 0.2724 (+/-0.0725)\n",
            "Iter 26600 - time: 7     - [train] loss: 0.3483 (+/-0.1338) - [valid] loss: 0.2468 (+/-0.0609) - [test] loss: 0.2613 (+/-0.0642)\n",
            "Iter 26700 - time: 7     - [train] loss: 0.2812 (+/-0.0728) - [valid] loss: 0.2217 (+/-0.0526) - [test] loss: 0.2197 (+/-0.0422)\n",
            "Iter 26800 - time: 7     - [train] loss: 0.1755 (+/-0.0453) - [valid] loss: 0.2103 (+/-0.0468) - [test] loss: 0.2087 (+/-0.0421)\n",
            "Iter 26900 - time: 7     - [train] loss: 0.2562 (+/-0.0657) - [valid] loss: 0.2596 (+/-0.0558) - [test] loss: 0.2248 (+/-0.0627)\n",
            "Iter 27000 - time: 7     - [train] loss: 0.1887 (+/-0.0381) - [valid] loss: 0.2179 (+/-0.0569) - [test] loss: 0.2292 (+/-0.0452)\n",
            "Iter 27100 - time: 7     - [train] loss: 0.2346 (+/-0.0624) - [valid] loss: 0.279  (+/-0.0562) - [test] loss: 0.3152 (+/-0.1005)\n",
            "Iter 27200 - time: 7     - [train] loss: 0.226  (+/-0.0473) - [valid] loss: 0.2082 (+/-0.0607) - [test] loss: 0.2068 (+/-0.0521)\n",
            "Iter 27300 - time: 7     - [train] loss: 0.2444 (+/-0.0658) - [valid] loss: 0.2983 (+/-0.0816) - [test] loss: 0.2148 (+/-0.0744)\n",
            "Iter 27400 - time: 7     - [train] loss: 0.1978 (+/-0.0438) - [valid] loss: 0.2217 (+/-0.0578) - [test] loss: 0.1999 (+/-0.0592)\n",
            "Iter 27500 - time: 7     - [train] loss: 0.2354 (+/-0.0796) - [valid] loss: 0.202  (+/-0.0598) - [test] loss: 0.2605 (+/-0.0674)\n",
            "Iter 27600 - time: 7     - [train] loss: 0.2245 (+/-0.0608) - [valid] loss: 0.191  (+/-0.0455) - [test] loss: 0.1848 (+/-0.0614)\n",
            "Iter 27700 - time: 8     - [train] loss: 0.2102 (+/-0.0523) - [valid] loss: 0.2019 (+/-0.0493) - [test] loss: 0.1835 (+/-0.0436)\n",
            "Iter 27800 - time: 7     - [train] loss: 0.2584 (+/-0.0539) - [valid] loss: 0.285  (+/-0.0891) - [test] loss: 0.2893 (+/-0.078 )\n",
            "Iter 27900 - time: 7     - [train] loss: 0.2564 (+/-0.0704) - [valid] loss: 0.2166 (+/-0.05  ) - [test] loss: 0.2629 (+/-0.0851)\n",
            "Iter 28000 - time: 7     - [train] loss: 0.2226 (+/-0.0486) - [valid] loss: 0.2293 (+/-0.0636) - [test] loss: 0.2286 (+/-0.0677)\n",
            "Iter 28100 - time: 7     - [train] loss: 0.2517 (+/-0.0598) - [valid] loss: 0.2243 (+/-0.0462) - [test] loss: 0.2096 (+/-0.0505)\n",
            "Iter 28200 - time: 7     - [train] loss: 0.2454 (+/-0.0656) - [valid] loss: 0.195  (+/-0.0404) - [test] loss: 0.244  (+/-0.0716)\n",
            "Iter 28300 - time: 7     - [train] loss: 0.1747 (+/-0.0371) - [valid] loss: 0.1781 (+/-0.0417) - [test] loss: 0.2094 (+/-0.0599)\n",
            "Iter 28400 - time: 7     - [train] loss: 0.2643 (+/-0.0931) - [valid] loss: 0.2403 (+/-0.0512) - [test] loss: 0.2934 (+/-0.0737)\n",
            "Iter 28500 - time: 7     - [train] loss: 0.2494 (+/-0.0656) - [valid] loss: 0.1992 (+/-0.0404) - [test] loss: 0.2173 (+/-0.0534)\n",
            "Iter 28600 - time: 7     - [train] loss: 0.274  (+/-0.062 ) - [valid] loss: 0.2493 (+/-0.0662) - [test] loss: 0.2877 (+/-0.0835)\n",
            "Iter 28700 - time: 7     - [train] loss: 0.2107 (+/-0.0441) - [valid] loss: 0.2102 (+/-0.0375) - [test] loss: 0.2538 (+/-0.0554)\n",
            "Iter 28800 - time: 7     - [train] loss: 0.2065 (+/-0.0581) - [valid] loss: 0.1944 (+/-0.0362) - [test] loss: 0.1932 (+/-0.0418)\n",
            "Iter 28900 - time: 7     - [train] loss: 0.1997 (+/-0.0431) - [valid] loss: 0.2438 (+/-0.0685) - [test] loss: 0.2085 (+/-0.0475)\n",
            "Iter 29000 - time: 7     - [train] loss: 0.3405 (+/-0.1208) - [valid] loss: 0.2408 (+/-0.0641) - [test] loss: 0.2442 (+/-0.0525)\n",
            "Iter 29100 - time: 7     - [train] loss: 0.2223 (+/-0.0552) - [valid] loss: 0.265  (+/-0.0695) - [test] loss: 0.2317 (+/-0.075 )\n",
            "Iter 29200 - time: 7     - [train] loss: 0.1989 (+/-0.039 ) - [valid] loss: 0.2122 (+/-0.0472) - [test] loss: 0.2631 (+/-0.0599)\n",
            "Iter 29300 - time: 7     - [train] loss: 0.2357 (+/-0.0535) - [valid] loss: 0.2184 (+/-0.0519) - [test] loss: 0.2396 (+/-0.0605)\n",
            "Iter 29400 - time: 7     - [train] loss: 0.2577 (+/-0.0641) - [valid] loss: 0.2093 (+/-0.0574) - [test] loss: 0.2545 (+/-0.07  )\n",
            "Iter 29500 - time: 7     - [train] loss: 0.2407 (+/-0.0679) - [valid] loss: 0.1892 (+/-0.0419) - [test] loss: 0.2554 (+/-0.0841)\n",
            "Iter 29600 - time: 7     - [train] loss: 0.2307 (+/-0.0647) - [valid] loss: 0.2373 (+/-0.059 ) - [test] loss: 0.2321 (+/-0.0593)\n",
            "Iter 29700 - time: 7     - [train] loss: 0.2261 (+/-0.055 ) - [valid] loss: 0.1977 (+/-0.0402) - [test] loss: 0.2159 (+/-0.0399)\n",
            "Iter 29800 - time: 7     - [train] loss: 0.2437 (+/-0.0501) - [valid] loss: 0.2161 (+/-0.0765) - [test] loss: 0.1621 (+/-0.0324)\n",
            "Iter 29900 - time: 7     - [train] loss: 0.2242 (+/-0.0675) - [valid] loss: 0.1832 (+/-0.0415) - [test] loss: 0.2197 (+/-0.055 )\n",
            "Iter 30000 - time: 7     - [train] loss: 0.2301 (+/-0.054 ) - [valid] loss: 0.1968 (+/-0.0417) - [test] loss: 0.2294 (+/-0.0593)\n",
            "Iter 30100 - time: 7     - [train] loss: 0.2084 (+/-0.0645) - [valid] loss: 0.1986 (+/-0.0377) - [test] loss: 0.2457 (+/-0.0652)\n",
            "Iter 30200 - time: 7     - [train] loss: 0.2163 (+/-0.0586) - [valid] loss: 0.2026 (+/-0.0392) - [test] loss: 0.2648 (+/-0.0625)\n",
            "Iter 30300 - time: 7     - [train] loss: 0.1987 (+/-0.0418) - [valid] loss: 0.2581 (+/-0.0686) - [test] loss: 0.1828 (+/-0.0431)\n",
            "Iter 30400 - time: 7     - [train] loss: 0.1557 (+/-0.0303) - [valid] loss: 0.2561 (+/-0.0551) - [test] loss: 0.1999 (+/-0.0477)\n",
            "Iter 30500 - time: 7     - [train] loss: 0.27   (+/-0.0761) - [valid] loss: 0.3148 (+/-0.0699) - [test] loss: 0.261  (+/-0.0678)\n",
            "Iter 30600 - time: 7     - [train] loss: 0.2672 (+/-0.0821) - [valid] loss: 0.2322 (+/-0.0533) - [test] loss: 0.1967 (+/-0.0415)\n",
            "Iter 30700 - time: 7     - [train] loss: 0.2021 (+/-0.0516) - [valid] loss: 0.1893 (+/-0.0409) - [test] loss: 0.2544 (+/-0.0682)\n",
            "Iter 30800 - time: 7     - [train] loss: 0.2453 (+/-0.0602) - [valid] loss: 0.2316 (+/-0.0519) - [test] loss: 0.2811 (+/-0.0812)\n",
            "Iter 30900 - time: 7     - [train] loss: 0.2181 (+/-0.0654) - [valid] loss: 0.2002 (+/-0.0516) - [test] loss: 0.1538 (+/-0.0342)\n",
            "Iter 31000 - time: 7     - [train] loss: 0.1982 (+/-0.0453) - [valid] loss: 0.257  (+/-0.0647) - [test] loss: 0.2812 (+/-0.0758)\n",
            "Iter 31100 - time: 7     - [train] loss: 0.2728 (+/-0.0571) - [valid] loss: 0.2119 (+/-0.053 ) - [test] loss: 0.2224 (+/-0.0426)\n",
            "Iter 31200 - time: 7     - [train] loss: 0.296  (+/-0.0763) - [valid] loss: 0.3181 (+/-0.0859) - [test] loss: 0.2824 (+/-0.0617)\n",
            "Iter 31300 - time: 7     - [train] loss: 0.3203 (+/-0.1294) - [valid] loss: 0.2115 (+/-0.0492) - [test] loss: 0.2159 (+/-0.0454)\n",
            "Iter 31400 - time: 7     - [train] loss: 0.3133 (+/-0.0803) - [valid] loss: 0.3023 (+/-0.0556) - [test] loss: 0.2671 (+/-0.0684)\n",
            "Iter 31500 - time: 7     - [train] loss: 0.2432 (+/-0.0658) - [valid] loss: 0.2117 (+/-0.0437) - [test] loss: 0.249  (+/-0.0813)\n",
            "Iter 31600 - time: 7     - [train] loss: 0.2518 (+/-0.0668) - [valid] loss: 0.3164 (+/-0.0809) - [test] loss: 0.2003 (+/-0.0476)\n",
            "Iter 31700 - time: 7     - [train] loss: 0.2239 (+/-0.0642) - [valid] loss: 0.2362 (+/-0.0594) - [test] loss: 0.2268 (+/-0.0548)\n",
            "Iter 31800 - time: 8     - [train] loss: 0.2379 (+/-0.0576) - [valid] loss: 0.1671 (+/-0.0375) - [test] loss: 0.2166 (+/-0.0639)\n",
            "Iter 31900 - time: 7     - [train] loss: 0.1906 (+/-0.0448) - [valid] loss: 0.2428 (+/-0.0516) - [test] loss: 0.2203 (+/-0.0445)\n",
            "Iter 32000 - time: 7     - [train] loss: 0.1938 (+/-0.047 ) - [valid] loss: 0.1882 (+/-0.0646) - [test] loss: 0.2026 (+/-0.0555)\n",
            "Iter 32100 - time: 7     - [train] loss: 0.2673 (+/-0.0631) - [valid] loss: 0.1965 (+/-0.0487) - [test] loss: 0.1773 (+/-0.0412)\n",
            "Iter 32200 - time: 7     - [train] loss: 0.1881 (+/-0.0469) - [valid] loss: 0.2188 (+/-0.0481) - [test] loss: 0.2169 (+/-0.0571)\n",
            "Iter 32300 - time: 7     - [train] loss: 0.2597 (+/-0.0737) - [valid] loss: 0.2226 (+/-0.0673) - [test] loss: 0.1945 (+/-0.0433)\n",
            "Iter 32400 - time: 7     - [train] loss: 0.2118 (+/-0.0528) - [valid] loss: 0.1547 (+/-0.033 ) - [test] loss: 0.2232 (+/-0.0713)\n",
            "Iter 32500 - time: 7     - [train] loss: 0.2495 (+/-0.0633) - [valid] loss: 0.1821 (+/-0.0476) - [test] loss: 0.2163 (+/-0.0552)\n",
            "Iter 32600 - time: 7     - [train] loss: 0.2214 (+/-0.0477) - [valid] loss: 0.2858 (+/-0.0876) - [test] loss: 0.2205 (+/-0.0377)\n",
            "Iter 32700 - time: 7     - [train] loss: 0.2097 (+/-0.0477) - [valid] loss: 0.2194 (+/-0.0444) - [test] loss: 0.2051 (+/-0.0564)\n",
            "Iter 32800 - time: 7     - [train] loss: 0.2171 (+/-0.0364) - [valid] loss: 0.2068 (+/-0.0518) - [test] loss: 0.2141 (+/-0.0549)\n",
            "Iter 32900 - time: 7     - [train] loss: 0.2086 (+/-0.0587) - [valid] loss: 0.2125 (+/-0.0532) - [test] loss: 0.2609 (+/-0.0702)\n",
            "Iter 33000 - time: 7     - [train] loss: 0.2429 (+/-0.0572) - [valid] loss: 0.2566 (+/-0.0508) - [test] loss: 0.1824 (+/-0.0414)\n",
            "Iter 33100 - time: 7     - [train] loss: 0.2089 (+/-0.0473) - [valid] loss: 0.2571 (+/-0.0863) - [test] loss: 0.1997 (+/-0.0561)\n",
            "Iter 33200 - time: 7     - [train] loss: 0.2183 (+/-0.0599) - [valid] loss: 0.2661 (+/-0.0899) - [test] loss: 0.2033 (+/-0.0439)\n",
            "Iter 33300 - time: 7     - [train] loss: 0.2165 (+/-0.0644) - [valid] loss: 0.2091 (+/-0.0546) - [test] loss: 0.1853 (+/-0.0363)\n",
            "Iter 33400 - time: 7     - [train] loss: 0.1997 (+/-0.0472) - [valid] loss: 0.167  (+/-0.0385) - [test] loss: 0.2341 (+/-0.0674)\n",
            "Iter 33500 - time: 7     - [train] loss: 0.2158 (+/-0.0361) - [valid] loss: 0.2098 (+/-0.0498) - [test] loss: 0.2514 (+/-0.0554)\n",
            "Iter 33600 - time: 7     - [train] loss: 0.2059 (+/-0.0535) - [valid] loss: 0.2254 (+/-0.0539) - [test] loss: 0.1854 (+/-0.0423)\n",
            "Iter 33700 - time: 7     - [train] loss: 0.2566 (+/-0.07  ) - [valid] loss: 0.1809 (+/-0.0391) - [test] loss: 0.1895 (+/-0.054 )\n",
            "Iter 33800 - time: 7     - [train] loss: 0.2225 (+/-0.0629) - [valid] loss: 0.2234 (+/-0.0616) - [test] loss: 0.2588 (+/-0.072 )\n",
            "Iter 33900 - time: 7     - [train] loss: 0.2121 (+/-0.046 ) - [valid] loss: 0.2125 (+/-0.0636) - [test] loss: 0.2428 (+/-0.0553)\n",
            "Iter 34000 - time: 7     - [train] loss: 0.2008 (+/-0.0444) - [valid] loss: 0.2337 (+/-0.0496) - [test] loss: 0.1775 (+/-0.0361)\n",
            "Iter 34100 - time: 7     - [train] loss: 0.2382 (+/-0.0529) - [valid] loss: 0.1918 (+/-0.0496) - [test] loss: 0.2711 (+/-0.0824)\n",
            "Iter 34200 - time: 7     - [train] loss: 0.2086 (+/-0.0515) - [valid] loss: 0.2271 (+/-0.0627) - [test] loss: 0.1868 (+/-0.0469)\n",
            "Iter 34300 - time: 7     - [train] loss: 0.2421 (+/-0.0526) - [valid] loss: 0.2193 (+/-0.0522) - [test] loss: 0.1601 (+/-0.0313)\n",
            "Iter 34400 - time: 7     - [train] loss: 0.2501 (+/-0.063 ) - [valid] loss: 0.1896 (+/-0.0553) - [test] loss: 0.2059 (+/-0.0555)\n",
            "Iter 34500 - time: 7     - [train] loss: 0.2585 (+/-0.0727) - [valid] loss: 0.2052 (+/-0.0522) - [test] loss: 0.2417 (+/-0.0514)\n",
            "Iter 34600 - time: 7     - [train] loss: 0.204  (+/-0.0452) - [valid] loss: 0.2198 (+/-0.0556) - [test] loss: 0.1946 (+/-0.0431)\n",
            "Iter 34700 - time: 7     - [train] loss: 0.1792 (+/-0.0489) - [valid] loss: 0.1915 (+/-0.0488) - [test] loss: 0.1999 (+/-0.0471)\n",
            "Iter 34800 - time: 7     - [train] loss: 0.2461 (+/-0.0615) - [valid] loss: 0.2297 (+/-0.0803) - [test] loss: 0.259  (+/-0.1234)\n",
            "Iter 34900 - time: 7     - [train] loss: 0.183  (+/-0.0385) - [valid] loss: 0.1685 (+/-0.0392) - [test] loss: 0.2307 (+/-0.0628)\n",
            "Iter 35000 - time: 7     - [train] loss: 0.2381 (+/-0.0635) - [valid] loss: 0.1698 (+/-0.0431) - [test] loss: 0.2171 (+/-0.0522)\n",
            "Iter 35100 - time: 7     - [train] loss: 0.1781 (+/-0.0307) - [valid] loss: 0.2469 (+/-0.0482) - [test] loss: 0.23   (+/-0.0597)\n",
            "Iter 35200 - time: 7     - [train] loss: 0.2183 (+/-0.0722) - [valid] loss: 0.2046 (+/-0.0537) - [test] loss: 0.1932 (+/-0.0502)\n",
            "Iter 35300 - time: 7     - [train] loss: 0.2656 (+/-0.0593) - [valid] loss: 0.2299 (+/-0.0408) - [test] loss: 0.2495 (+/-0.0521)\n",
            "Iter 35400 - time: 7     - [train] loss: 0.3241 (+/-0.0852) - [valid] loss: 0.2688 (+/-0.0877) - [test] loss: 0.223  (+/-0.0443)\n",
            "Iter 35500 - time: 7     - [train] loss: 0.1847 (+/-0.0478) - [valid] loss: 0.1874 (+/-0.0457) - [test] loss: 0.2127 (+/-0.0502)\n",
            "Iter 35600 - time: 7     - [train] loss: 0.28   (+/-0.0648) - [valid] loss: 0.291  (+/-0.0699) - [test] loss: 0.2572 (+/-0.0499)\n",
            "Iter 35700 - time: 7     - [train] loss: 0.192  (+/-0.0493) - [valid] loss: 0.1905 (+/-0.0514) - [test] loss: 0.2522 (+/-0.0849)\n",
            "Iter 35800 - time: 7     - [train] loss: 0.1639 (+/-0.0386) - [valid] loss: 0.2099 (+/-0.063 ) - [test] loss: 0.2291 (+/-0.0595)\n",
            "Iter 35900 - time: 8     - [train] loss: 0.2377 (+/-0.0543) - [valid] loss: 0.2247 (+/-0.0488) - [test] loss: 0.2188 (+/-0.0504)\n",
            "Iter 36000 - time: 7     - [train] loss: 0.2774 (+/-0.0758) - [valid] loss: 0.1872 (+/-0.0445) - [test] loss: 0.2584 (+/-0.0653)\n",
            "Iter 36100 - time: 7     - [train] loss: 0.1612 (+/-0.0367) - [valid] loss: 0.2118 (+/-0.0565) - [test] loss: 0.1833 (+/-0.047 )\n",
            "Iter 36200 - time: 7     - [train] loss: 0.2308 (+/-0.0479) - [valid] loss: 0.1761 (+/-0.0455) - [test] loss: 0.2678 (+/-0.0744)\n",
            "Iter 36300 - time: 7     - [train] loss: 0.2533 (+/-0.0829) - [valid] loss: 0.1738 (+/-0.0443) - [test] loss: 0.2149 (+/-0.0562)\n",
            "Iter 36400 - time: 7     - [train] loss: 0.1803 (+/-0.0424) - [valid] loss: 0.1963 (+/-0.0428) - [test] loss: 0.2344 (+/-0.0568)\n",
            "Iter 36500 - time: 7     - [train] loss: 0.2303 (+/-0.0633) - [valid] loss: 0.2001 (+/-0.0527) - [test] loss: 0.223  (+/-0.0593)\n",
            "Iter 36600 - time: 7     - [train] loss: 0.2022 (+/-0.047 ) - [valid] loss: 0.1942 (+/-0.0568) - [test] loss: 0.2196 (+/-0.0516)\n",
            "Iter 36700 - time: 7     - [train] loss: 0.1806 (+/-0.0357) - [valid] loss: 0.2166 (+/-0.0462) - [test] loss: 0.1828 (+/-0.0498)\n",
            "Iter 36800 - time: 7     - [train] loss: 0.2762 (+/-0.0784) - [valid] loss: 0.241  (+/-0.042 ) - [test] loss: 0.2553 (+/-0.0447)\n",
            "Iter 36900 - time: 7     - [train] loss: 0.2425 (+/-0.0595) - [valid] loss: 0.2677 (+/-0.0522) - [test] loss: 0.2528 (+/-0.0543)\n",
            "Iter 37000 - time: 7     - [train] loss: 0.214  (+/-0.052 ) - [valid] loss: 0.213  (+/-0.0485) - [test] loss: 0.3233 (+/-0.0776)\n",
            "Iter 37100 - time: 7     - [train] loss: 0.2011 (+/-0.0465) - [valid] loss: 0.2642 (+/-0.0662) - [test] loss: 0.2276 (+/-0.07  )\n",
            "Iter 37200 - time: 7     - [train] loss: 0.2839 (+/-0.0724) - [valid] loss: 0.1955 (+/-0.035 ) - [test] loss: 0.2293 (+/-0.0472)\n",
            "Iter 37300 - time: 7     - [train] loss: 0.1998 (+/-0.0529) - [valid] loss: 0.1593 (+/-0.0358) - [test] loss: 0.2152 (+/-0.0619)\n",
            "Iter 37400 - time: 7     - [train] loss: 0.2591 (+/-0.0547) - [valid] loss: 0.2909 (+/-0.0709) - [test] loss: 0.2772 (+/-0.0577)\n",
            "Iter 37500 - time: 7     - [train] loss: 0.2222 (+/-0.0367) - [valid] loss: 0.2748 (+/-0.0408) - [test] loss: 0.2291 (+/-0.0376)\n",
            "saving best model at iter 37600\n",
            "Iter 37600 - time: 7     - [train] loss: 0.1852 (+/-0.0546) - [valid] loss: 0.146  (+/-0.0327) - [test] loss: 0.1485 (+/-0.0342)\n",
            "Iter 37700 - time: 7     - [train] loss: 0.163  (+/-0.0374) - [valid] loss: 0.216  (+/-0.0495) - [test] loss: 0.1665 (+/-0.0346)\n",
            "Iter 37800 - time: 7     - [train] loss: 0.2501 (+/-0.0554) - [valid] loss: 0.2194 (+/-0.0499) - [test] loss: 0.2052 (+/-0.0453)\n",
            "Iter 37900 - time: 7     - [train] loss: 0.2209 (+/-0.0698) - [valid] loss: 0.2136 (+/-0.0475) - [test] loss: 0.1949 (+/-0.0512)\n",
            "Iter 38000 - time: 7     - [train] loss: 0.2819 (+/-0.0607) - [valid] loss: 0.2262 (+/-0.0532) - [test] loss: 0.2354 (+/-0.055 )\n",
            "Iter 38100 - time: 7     - [train] loss: 0.2426 (+/-0.054 ) - [valid] loss: 0.2184 (+/-0.0637) - [test] loss: 0.2358 (+/-0.0686)\n",
            "Iter 38200 - time: 7     - [train] loss: 0.2154 (+/-0.0495) - [valid] loss: 0.2236 (+/-0.0419) - [test] loss: 0.2205 (+/-0.0497)\n",
            "Iter 38300 - time: 7     - [train] loss: 0.2416 (+/-0.0603) - [valid] loss: 0.1856 (+/-0.045 ) - [test] loss: 0.2065 (+/-0.0522)\n",
            "Iter 38400 - time: 7     - [train] loss: 0.2231 (+/-0.0556) - [valid] loss: 0.1707 (+/-0.0377) - [test] loss: 0.2327 (+/-0.0993)\n",
            "Iter 38500 - time: 7     - [train] loss: 0.2644 (+/-0.0706) - [valid] loss: 0.3134 (+/-0.0829) - [test] loss: 0.2547 (+/-0.0592)\n",
            "Iter 38600 - time: 7     - [train] loss: 0.2375 (+/-0.0605) - [valid] loss: 0.254  (+/-0.0752) - [test] loss: 0.2081 (+/-0.0523)\n",
            "Iter 38700 - time: 7     - [train] loss: 0.2242 (+/-0.0449) - [valid] loss: 0.1816 (+/-0.045 ) - [test] loss: 0.2616 (+/-0.0626)\n",
            "Iter 38800 - time: 7     - [train] loss: 0.1931 (+/-0.0536) - [valid] loss: 0.1562 (+/-0.0346) - [test] loss: 0.2167 (+/-0.068 )\n",
            "Iter 38900 - time: 7     - [train] loss: 0.2388 (+/-0.0567) - [valid] loss: 0.2454 (+/-0.0517) - [test] loss: 0.2168 (+/-0.045 )\n",
            "Iter 39000 - time: 7     - [train] loss: 0.2535 (+/-0.102 ) - [valid] loss: 0.2275 (+/-0.0868) - [test] loss: 0.1853 (+/-0.0415)\n",
            "Iter 39100 - time: 7     - [train] loss: 0.1712 (+/-0.0326) - [valid] loss: 0.1962 (+/-0.0596) - [test] loss: 0.1876 (+/-0.0537)\n",
            "Iter 39200 - time: 7     - [train] loss: 0.22   (+/-0.0493) - [valid] loss: 0.226  (+/-0.0553) - [test] loss: 0.2756 (+/-0.072 )\n",
            "Iter 39300 - time: 7     - [train] loss: 0.197  (+/-0.0484) - [valid] loss: 0.1859 (+/-0.0451) - [test] loss: 0.238  (+/-0.0579)\n",
            "Iter 39400 - time: 7     - [train] loss: 0.1969 (+/-0.0402) - [valid] loss: 0.1961 (+/-0.0531) - [test] loss: 0.2294 (+/-0.0527)\n",
            "Iter 39500 - time: 7     - [train] loss: 0.2351 (+/-0.0553) - [valid] loss: 0.2033 (+/-0.0609) - [test] loss: 0.2164 (+/-0.0512)\n",
            "Iter 39600 - time: 7     - [train] loss: 0.2204 (+/-0.0475) - [valid] loss: 0.1979 (+/-0.0582) - [test] loss: 0.1926 (+/-0.0456)\n",
            "Iter 39700 - time: 7     - [train] loss: 0.2292 (+/-0.0552) - [valid] loss: 0.1578 (+/-0.0324) - [test] loss: 0.1971 (+/-0.0369)\n",
            "Iter 39800 - time: 7     - [train] loss: 0.2122 (+/-0.0553) - [valid] loss: 0.1941 (+/-0.046 ) - [test] loss: 0.2362 (+/-0.0508)\n",
            "Iter 39900 - time: 7     - [train] loss: 0.2579 (+/-0.0524) - [valid] loss: 0.2189 (+/-0.0401) - [test] loss: 0.3042 (+/-0.0741)\n",
            "Iter 40000 - time: 8     - [train] loss: 0.1972 (+/-0.0501) - [valid] loss: 0.2075 (+/-0.0398) - [test] loss: 0.2264 (+/-0.0637)\n",
            "Iter 40100 - time: 7     - [train] loss: 0.2997 (+/-0.0855) - [valid] loss: 0.2611 (+/-0.0667) - [test] loss: 0.2885 (+/-0.0874)\n",
            "Iter 40200 - time: 7     - [train] loss: 0.2244 (+/-0.0547) - [valid] loss: 0.2103 (+/-0.0479) - [test] loss: 0.2099 (+/-0.0472)\n",
            "Iter 40300 - time: 7     - [train] loss: 0.2534 (+/-0.0541) - [valid] loss: 0.2704 (+/-0.0655) - [test] loss: 0.1771 (+/-0.0443)\n",
            "Iter 40400 - time: 7     - [train] loss: 0.3248 (+/-0.0827) - [valid] loss: 0.2956 (+/-0.071 ) - [test] loss: 0.2635 (+/-0.0725)\n",
            "Iter 40500 - time: 7     - [train] loss: 0.1986 (+/-0.0661) - [valid] loss: 0.1853 (+/-0.0512) - [test] loss: 0.2096 (+/-0.042 )\n",
            "Iter 40600 - time: 7     - [train] loss: 0.2616 (+/-0.0987) - [valid] loss: 0.2582 (+/-0.0671) - [test] loss: 0.2225 (+/-0.0617)\n",
            "Iter 40700 - time: 7     - [train] loss: 0.209  (+/-0.0504) - [valid] loss: 0.3099 (+/-0.0809) - [test] loss: 0.1904 (+/-0.0627)\n",
            "Iter 40800 - time: 7     - [train] loss: 0.199  (+/-0.0434) - [valid] loss: 0.2222 (+/-0.0582) - [test] loss: 0.2101 (+/-0.0538)\n",
            "Iter 40900 - time: 7     - [train] loss: 0.2144 (+/-0.049 ) - [valid] loss: 0.1797 (+/-0.0455) - [test] loss: 0.1913 (+/-0.0437)\n",
            "Iter 41000 - time: 7     - [train] loss: 0.2004 (+/-0.0472) - [valid] loss: 0.2141 (+/-0.0562) - [test] loss: 0.1968 (+/-0.0471)\n",
            "Iter 41100 - time: 7     - [train] loss: 0.2049 (+/-0.0594) - [valid] loss: 0.2849 (+/-0.0633) - [test] loss: 0.2064 (+/-0.0634)\n",
            "Iter 41200 - time: 7     - [train] loss: 0.2389 (+/-0.0658) - [valid] loss: 0.1929 (+/-0.0713) - [test] loss: 0.1545 (+/-0.031 )\n",
            "Iter 41300 - time: 7     - [train] loss: 0.2165 (+/-0.0578) - [valid] loss: 0.255  (+/-0.0696) - [test] loss: 0.184  (+/-0.043 )\n",
            "Iter 41400 - time: 7     - [train] loss: 0.2248 (+/-0.0543) - [valid] loss: 0.2558 (+/-0.0701) - [test] loss: 0.2326 (+/-0.0512)\n",
            "Iter 41500 - time: 7     - [train] loss: 0.236  (+/-0.0553) - [valid] loss: 0.191  (+/-0.0429) - [test] loss: 0.1822 (+/-0.0662)\n",
            "Iter 41600 - time: 7     - [train] loss: 0.2279 (+/-0.053 ) - [valid] loss: 0.1787 (+/-0.0433) - [test] loss: 0.2223 (+/-0.0419)\n",
            "Iter 41700 - time: 7     - [train] loss: 0.2267 (+/-0.0726) - [valid] loss: 0.1971 (+/-0.0511) - [test] loss: 0.2125 (+/-0.052 )\n",
            "Iter 41800 - time: 7     - [train] loss: 0.2046 (+/-0.0412) - [valid] loss: 0.2157 (+/-0.0566) - [test] loss: 0.2095 (+/-0.0551)\n",
            "Iter 41900 - time: 7     - [train] loss: 0.2473 (+/-0.0821) - [valid] loss: 0.2334 (+/-0.0682) - [test] loss: 0.2005 (+/-0.0446)\n",
            "Iter 42000 - time: 7     - [train] loss: 0.2169 (+/-0.0461) - [valid] loss: 0.2113 (+/-0.0479) - [test] loss: 0.1918 (+/-0.0426)\n",
            "Iter 42100 - time: 7     - [train] loss: 0.1747 (+/-0.033 ) - [valid] loss: 0.207  (+/-0.0508) - [test] loss: 0.1749 (+/-0.0411)\n",
            "Iter 42200 - time: 7     - [train] loss: 0.2667 (+/-0.0525) - [valid] loss: 0.2672 (+/-0.0572) - [test] loss: 0.3365 (+/-0.0659)\n",
            "Iter 42300 - time: 7     - [train] loss: 0.2018 (+/-0.0458) - [valid] loss: 0.255  (+/-0.0704) - [test] loss: 0.2363 (+/-0.0623)\n",
            "Iter 42400 - time: 7     - [train] loss: 0.2406 (+/-0.0732) - [valid] loss: 0.2231 (+/-0.0709) - [test] loss: 0.2204 (+/-0.0501)\n",
            "Iter 42500 - time: 7     - [train] loss: 0.2057 (+/-0.0495) - [valid] loss: 0.2039 (+/-0.0516) - [test] loss: 0.2473 (+/-0.0674)\n",
            "Iter 42600 - time: 7     - [train] loss: 0.1889 (+/-0.0559) - [valid] loss: 0.2092 (+/-0.0415) - [test] loss: 0.2452 (+/-0.0772)\n",
            "Iter 42700 - time: 7     - [train] loss: 0.2301 (+/-0.0553) - [valid] loss: 0.2116 (+/-0.0399) - [test] loss: 0.1792 (+/-0.0403)\n",
            "Iter 42800 - time: 7     - [train] loss: 0.2773 (+/-0.0748) - [valid] loss: 0.215  (+/-0.0509) - [test] loss: 0.2352 (+/-0.0548)\n",
            "Iter 42900 - time: 7     - [train] loss: 0.1807 (+/-0.0459) - [valid] loss: 0.2137 (+/-0.0477) - [test] loss: 0.2489 (+/-0.0868)\n",
            "Iter 43000 - time: 7     - [train] loss: 0.2604 (+/-0.0599) - [valid] loss: 0.2913 (+/-0.0795) - [test] loss: 0.2693 (+/-0.0848)\n",
            "Iter 43100 - time: 7     - [train] loss: 0.2348 (+/-0.0527) - [valid] loss: 0.2242 (+/-0.0586) - [test] loss: 0.1966 (+/-0.0448)\n",
            "Iter 43200 - time: 7     - [train] loss: 0.1992 (+/-0.0459) - [valid] loss: 0.1733 (+/-0.0462) - [test] loss: 0.1891 (+/-0.0509)\n",
            "Iter 43300 - time: 7     - [train] loss: 0.2194 (+/-0.0546) - [valid] loss: 0.2174 (+/-0.0508) - [test] loss: 0.2435 (+/-0.0674)\n",
            "Iter 43400 - time: 7     - [train] loss: 0.2039 (+/-0.052 ) - [valid] loss: 0.1947 (+/-0.0508) - [test] loss: 0.2205 (+/-0.0555)\n",
            "Iter 43500 - time: 7     - [train] loss: 0.2122 (+/-0.0568) - [valid] loss: 0.2093 (+/-0.0589) - [test] loss: 0.1907 (+/-0.0379)\n",
            "Iter 43600 - time: 7     - [train] loss: 0.1825 (+/-0.0473) - [valid] loss: 0.2082 (+/-0.0447) - [test] loss: 0.2039 (+/-0.0584)\n",
            "Iter 43700 - time: 7     - [train] loss: 0.186  (+/-0.0376) - [valid] loss: 0.1593 (+/-0.0383) - [test] loss: 0.2221 (+/-0.0609)\n",
            "Iter 43800 - time: 7     - [train] loss: 0.2377 (+/-0.0612) - [valid] loss: 0.1911 (+/-0.0391) - [test] loss: 0.2243 (+/-0.0641)\n",
            "Iter 43900 - time: 7     - [train] loss: 0.2386 (+/-0.0557) - [valid] loss: 0.2386 (+/-0.0546) - [test] loss: 0.2235 (+/-0.0483)\n",
            "Iter 44000 - time: 7     - [train] loss: 0.2248 (+/-0.0735) - [valid] loss: 0.2346 (+/-0.0485) - [test] loss: 0.2142 (+/-0.059 )\n",
            "Iter 44100 - time: 8     - [train] loss: 0.2229 (+/-0.075 ) - [valid] loss: 0.2816 (+/-0.0732) - [test] loss: 0.2929 (+/-0.0788)\n",
            "Iter 44200 - time: 7     - [train] loss: 0.1929 (+/-0.045 ) - [valid] loss: 0.2667 (+/-0.0751) - [test] loss: 0.2628 (+/-0.0862)\n",
            "Iter 44300 - time: 7     - [train] loss: 0.2023 (+/-0.0522) - [valid] loss: 0.1796 (+/-0.0409) - [test] loss: 0.2187 (+/-0.0473)\n",
            "Iter 44400 - time: 7     - [train] loss: 0.2242 (+/-0.0592) - [valid] loss: 0.2639 (+/-0.09  ) - [test] loss: 0.234  (+/-0.0567)\n",
            "Iter 44500 - time: 7     - [train] loss: 0.1867 (+/-0.0488) - [valid] loss: 0.2524 (+/-0.0736) - [test] loss: 0.2375 (+/-0.0545)\n",
            "Iter 44600 - time: 7     - [train] loss: 0.2196 (+/-0.067 ) - [valid] loss: 0.2192 (+/-0.0483) - [test] loss: 0.1677 (+/-0.0309)\n",
            "Iter 44700 - time: 7     - [train] loss: 0.218  (+/-0.0509) - [valid] loss: 0.2079 (+/-0.0392) - [test] loss: 0.226  (+/-0.0517)\n",
            "Iter 44800 - time: 7     - [train] loss: 0.1832 (+/-0.0495) - [valid] loss: 0.2114 (+/-0.0626) - [test] loss: 0.218  (+/-0.0485)\n",
            "Iter 44900 - time: 7     - [train] loss: 0.1534 (+/-0.0405) - [valid] loss: 0.2014 (+/-0.0452) - [test] loss: 0.2316 (+/-0.0655)\n",
            "Iter 45000 - time: 7     - [train] loss: 0.2262 (+/-0.0839) - [valid] loss: 0.1853 (+/-0.0423) - [test] loss: 0.1999 (+/-0.048 )\n",
            "Iter 45100 - time: 7     - [train] loss: 0.2509 (+/-0.0671) - [valid] loss: 0.2536 (+/-0.0534) - [test] loss: 0.29   (+/-0.0782)\n",
            "Iter 45200 - time: 7     - [train] loss: 0.2063 (+/-0.051 ) - [valid] loss: 0.2696 (+/-0.061 ) - [test] loss: 0.2092 (+/-0.0487)\n",
            "Iter 45300 - time: 7     - [train] loss: 0.1889 (+/-0.0564) - [valid] loss: 0.2247 (+/-0.0558) - [test] loss: 0.2492 (+/-0.0877)\n",
            "Iter 45400 - time: 7     - [train] loss: 0.2116 (+/-0.0553) - [valid] loss: 0.1904 (+/-0.0526) - [test] loss: 0.2642 (+/-0.0637)\n",
            "Iter 45500 - time: 7     - [train] loss: 0.1829 (+/-0.0478) - [valid] loss: 0.2318 (+/-0.0904) - [test] loss: 0.2286 (+/-0.068 )\n",
            "Iter 45600 - time: 7     - [train] loss: 0.2547 (+/-0.072 ) - [valid] loss: 0.2189 (+/-0.0485) - [test] loss: 0.1833 (+/-0.0354)\n",
            "Iter 45700 - time: 7     - [train] loss: 0.246  (+/-0.0508) - [valid] loss: 0.2655 (+/-0.0642) - [test] loss: 0.2189 (+/-0.0551)\n",
            "Iter 45800 - time: 7     - [train] loss: 0.169  (+/-0.0403) - [valid] loss: 0.1823 (+/-0.0428) - [test] loss: 0.2106 (+/-0.0517)\n",
            "Iter 45900 - time: 7     - [train] loss: 0.2255 (+/-0.0481) - [valid] loss: 0.2663 (+/-0.0636) - [test] loss: 0.2534 (+/-0.0663)\n",
            "Iter 46000 - time: 7     - [train] loss: 0.2264 (+/-0.0403) - [valid] loss: 0.2253 (+/-0.0457) - [test] loss: 0.2498 (+/-0.0505)\n",
            "Iter 46100 - time: 7     - [train] loss: 0.1842 (+/-0.0458) - [valid] loss: 0.2452 (+/-0.0681) - [test] loss: 0.1787 (+/-0.0432)\n",
            "Iter 46200 - time: 7     - [train] loss: 0.2174 (+/-0.065 ) - [valid] loss: 0.2743 (+/-0.1219) - [test] loss: 0.2289 (+/-0.0527)\n",
            "Iter 46300 - time: 7     - [train] loss: 0.2532 (+/-0.077 ) - [valid] loss: 0.2227 (+/-0.0602) - [test] loss: 0.2025 (+/-0.0537)\n",
            "Iter 46400 - time: 7     - [train] loss: 0.2397 (+/-0.0458) - [valid] loss: 0.2012 (+/-0.0352) - [test] loss: 0.2406 (+/-0.0563)\n",
            "Iter 46500 - time: 7     - [train] loss: 0.212  (+/-0.0687) - [valid] loss: 0.2265 (+/-0.0539) - [test] loss: 0.1906 (+/-0.0387)\n",
            "Iter 46600 - time: 7     - [train] loss: 0.1841 (+/-0.0414) - [valid] loss: 0.2508 (+/-0.0765) - [test] loss: 0.1863 (+/-0.0457)\n",
            "Iter 46700 - time: 7     - [train] loss: 0.2397 (+/-0.058 ) - [valid] loss: 0.1994 (+/-0.0482) - [test] loss: 0.2073 (+/-0.0567)\n",
            "Iter 46800 - time: 7     - [train] loss: 0.1658 (+/-0.0337) - [valid] loss: 0.2012 (+/-0.0539) - [test] loss: 0.2615 (+/-0.0643)\n",
            "Iter 46900 - time: 7     - [train] loss: 0.243  (+/-0.0644) - [valid] loss: 0.1823 (+/-0.043 ) - [test] loss: 0.2093 (+/-0.0586)\n",
            "Iter 47000 - time: 7     - [train] loss: 0.1992 (+/-0.05  ) - [valid] loss: 0.2287 (+/-0.053 ) - [test] loss: 0.2136 (+/-0.0548)\n",
            "Iter 47100 - time: 7     - [train] loss: 0.2408 (+/-0.059 ) - [valid] loss: 0.173  (+/-0.0356) - [test] loss: 0.2174 (+/-0.0599)\n",
            "Iter 47200 - time: 7     - [train] loss: 0.2162 (+/-0.0615) - [valid] loss: 0.2535 (+/-0.0748) - [test] loss: 0.2419 (+/-0.0618)\n",
            "Iter 47300 - time: 7     - [train] loss: 0.2085 (+/-0.0426) - [valid] loss: 0.2226 (+/-0.051 ) - [test] loss: 0.1883 (+/-0.0455)\n",
            "Iter 47400 - time: 7     - [train] loss: 0.1921 (+/-0.04  ) - [valid] loss: 0.2488 (+/-0.0756) - [test] loss: 0.1945 (+/-0.06  )\n",
            "Iter 47500 - time: 7     - [train] loss: 0.2174 (+/-0.043 ) - [valid] loss: 0.2596 (+/-0.0506) - [test] loss: 0.222  (+/-0.0501)\n",
            "Iter 47600 - time: 7     - [train] loss: 0.2391 (+/-0.0541) - [valid] loss: 0.2063 (+/-0.0491) - [test] loss: 0.1513 (+/-0.026 )\n",
            "Iter 47700 - time: 7     - [train] loss: 0.169  (+/-0.0419) - [valid] loss: 0.2241 (+/-0.0553) - [test] loss: 0.2177 (+/-0.0512)\n",
            "Iter 47800 - time: 7     - [train] loss: 0.1874 (+/-0.0514) - [valid] loss: 0.2612 (+/-0.0683) - [test] loss: 0.1783 (+/-0.0482)\n",
            "Iter 47900 - time: 7     - [train] loss: 0.2144 (+/-0.0566) - [valid] loss: 0.2154 (+/-0.0455) - [test] loss: 0.2285 (+/-0.0671)\n",
            "Iter 48000 - time: 7     - [train] loss: 0.2027 (+/-0.0548) - [valid] loss: 0.238  (+/-0.087 ) - [test] loss: 0.1911 (+/-0.0521)\n",
            "Iter 48100 - time: 7     - [train] loss: 0.2004 (+/-0.0631) - [valid] loss: 0.1752 (+/-0.04  ) - [test] loss: 0.2028 (+/-0.0557)\n",
            "Iter 48200 - time: 7     - [train] loss: 0.2454 (+/-0.0635) - [valid] loss: 0.2085 (+/-0.0464) - [test] loss: 0.2179 (+/-0.0504)\n",
            "Iter 48300 - time: 7     - [train] loss: 0.2514 (+/-0.054 ) - [valid] loss: 0.2672 (+/-0.0602) - [test] loss: 0.2482 (+/-0.0613)\n",
            "Iter 48400 - time: 7     - [train] loss: 0.2151 (+/-0.0454) - [valid] loss: 0.2541 (+/-0.0854) - [test] loss: 0.2014 (+/-0.0481)\n",
            "Iter 48500 - time: 7     - [train] loss: 0.1939 (+/-0.0437) - [valid] loss: 0.237  (+/-0.0533) - [test] loss: 0.1921 (+/-0.0536)\n",
            "Iter 48600 - time: 7     - [train] loss: 0.2135 (+/-0.0564) - [valid] loss: 0.248  (+/-0.0686) - [test] loss: 0.2404 (+/-0.0515)\n",
            "Iter 48700 - time: 7     - [train] loss: 0.1893 (+/-0.0444) - [valid] loss: 0.2545 (+/-0.0679) - [test] loss: 0.2597 (+/-0.1056)\n",
            "Iter 48800 - time: 7     - [train] loss: 0.1965 (+/-0.0475) - [valid] loss: 0.2615 (+/-0.0779) - [test] loss: 0.2199 (+/-0.0547)\n",
            "Iter 48900 - time: 7     - [train] loss: 0.2294 (+/-0.0742) - [valid] loss: 0.1843 (+/-0.0367) - [test] loss: 0.1552 (+/-0.0299)\n",
            "Iter 49000 - time: 7     - [train] loss: 0.2388 (+/-0.0538) - [valid] loss: 0.2929 (+/-0.0831) - [test] loss: 0.2101 (+/-0.0476)\n",
            "Iter 49100 - time: 7     - [train] loss: 0.2717 (+/-0.0824) - [valid] loss: 0.2503 (+/-0.0596) - [test] loss: 0.1976 (+/-0.0482)\n",
            "Iter 49200 - time: 8     - [train] loss: 0.1985 (+/-0.0528) - [valid] loss: 0.1753 (+/-0.0353) - [test] loss: 0.2231 (+/-0.067 )\n",
            "Iter 49300 - time: 7     - [train] loss: 0.2382 (+/-0.0585) - [valid] loss: 0.2408 (+/-0.062 ) - [test] loss: 0.2628 (+/-0.0551)\n",
            "Iter 49400 - time: 7     - [train] loss: 0.2574 (+/-0.0787) - [valid] loss: 0.2063 (+/-0.0404) - [test] loss: 0.1913 (+/-0.0463)\n",
            "Iter 49500 - time: 7     - [train] loss: 0.2459 (+/-0.0559) - [valid] loss: 0.2193 (+/-0.0665) - [test] loss: 0.2136 (+/-0.0609)\n",
            "Iter 49600 - time: 7     - [train] loss: 0.3127 (+/-0.1005) - [valid] loss: 0.2651 (+/-0.059 ) - [test] loss: 0.2348 (+/-0.0531)\n",
            "Iter 49700 - time: 7     - [train] loss: 0.2492 (+/-0.0639) - [valid] loss: 0.2084 (+/-0.0507) - [test] loss: 0.1934 (+/-0.0503)\n",
            "Iter 49800 - time: 7     - [train] loss: 0.172  (+/-0.045 ) - [valid] loss: 0.1924 (+/-0.038 ) - [test] loss: 0.2167 (+/-0.0738)\n",
            "Iter 49900 - time: 7     - [train] loss: 0.2481 (+/-0.057 ) - [valid] loss: 0.2171 (+/-0.0438) - [test] loss: 0.2351 (+/-0.0479)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHe4VPgNEJv3",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkAITogBEKBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.path.realpath(__file__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdxr1xazEL3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}